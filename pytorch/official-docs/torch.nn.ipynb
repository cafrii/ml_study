{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ec9261",
   "metadata": {},
   "source": [
    "https://docs.pytorch.org/docs/stable/nn.html\n",
    "\n",
    "docs/torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4037b953",
   "metadata": {},
   "source": [
    "## Containers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45367aa3",
   "metadata": {},
   "source": [
    "## Convolution Layers\n",
    "\n",
    "### nn.Conv2d\n",
    "ì—¬ëŸ¬ ê°œì˜ ì…ë ¥ í‰ë©´ìœ¼ë¡œ êµ¬ì„±ëœ ì…ë ¥ ì‹ í˜¸ì— 2D í•©ì„±ê³±ì„ ì ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c36a73",
   "metadata": {},
   "source": [
    "```\n",
    "class torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                      stride=1, padding=0, dilation=1, groups=1,\n",
    "                      bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc67ae5a",
   "metadata": {},
   "source": [
    "https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d\n",
    "\n",
    "ì—¬ëŸ¬ ì…ë ¥ í‰ë©´ìœ¼ë¡œ êµ¬ì„±ëœ ì…ë ¥ ì‹ í˜¸ì— 2D ì»¨ë³¼ë£¨ì…˜ì„ ì ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ê°€ì¥ ê°„ë‹¨í•œ ê²½ìš°, ì…ë ¥ í¬ê¸° $(N, C_{\\text{in}}, H, W)$, ê·¸ë¦¬ê³  ì¶œë ¥ $(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ ì— ë”°ë¥¸ ë ˆì´ì–´ì˜ ì¶œë ¥ ê°’ì€ ì •í™•í•˜ê²Œ ë‹¤ìŒê³¼ ê°™ì´ ê¸°ìˆ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "$$ \\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) +\n",
    "\\sum_{k = 0}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k) $$\n",
    "\n",
    "ì—¬ê¸°ì„œ $\\star$ ëŠ” ìœ íš¨í•œ 2D êµì°¨ ìƒê´€ ì—°ì‚°ì (cross-correlation operator), ğ‘ ì€ ë°°ì¹˜ í¬ê¸°, ğ¶ ëŠ” ì±„ë„ ìˆ˜, ğ» ëŠ” ì…ë ¥ í‰ë©´ì˜ ë†’ì´(í”½ì…€), ğ‘Š ëŠ” í­(í”½ì…€)ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ëª¨ë“ˆì€ TensorFloat32 ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.\n",
    "\n",
    "íŠ¹ì • ROCm ì¥ì¹˜ì—ì„œ float16 ì…ë ¥ì„ ì‚¬ìš©í•  ë•Œ, ì´ ëª¨ë“ˆì€ ì—­ë°©í–¥ì— ëŒ€í•´ ë‹¤ë¥¸ ì •ë°€ë„ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `stride` ëŠ” êµì°¨ ìƒê´€ê´€ê³„ì˜ ë³´í­, ë‹¨ì¼ ìˆ«ì ë˜ëŠ” íŠœí”Œì„ ì œì–´í•©ë‹ˆë‹¤.\n",
    "- `padding` ì€ ì…ë ¥ì— ì ìš©ë˜ëŠ” íŒ¨ë”©ì˜ ì–‘ì„ ì œì–´í•©ë‹ˆë‹¤. ë¬¸ìì—´ {'valid', 'same'} ì´ê±°ë‚˜ ì–‘ìª½ì— ì ìš©ë˜ëŠ” ì•”ì‹œì  íŒ¨ë”©ì˜ ì–‘ì„ ë‚˜íƒ€ë‚´ëŠ” ì •ìˆ˜/ì •ìˆ˜ íŠœí”Œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- `dilation` ì€ ì»¤ë„ í¬ì¸íŠ¸ ì‚¬ì´ì˜ ê°„ê²©ì„ ì œì–´í•˜ë©°, a` trous (ì•„íŠ¸ë¥´) ì•Œê³ ë¦¬ì¦˜ì´ë¼ê³ ë„ í•©ë‹ˆë‹¤. ì„¤ëª…í•˜ê¸°ëŠ” ì–´ë µì§€ë§Œ ì´ ë§í¬ì— í™•ì¥ ê¸°ëŠ¥ì„ ì‹œê°í™”í•œ ë©‹ì§„ ê·¸ë¦¼ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "  - <img width=\"100\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/dilation.gif\">\n",
    "- `groups` ëŠ” ì…ë ¥ê³¼ ì¶œë ¥ ê°„ì˜ ì—°ê²°ì„ ì œì–´í•©ë‹ˆë‹¤. `in_channels`ì™€ `out_channels`ëŠ” ëª¨ë‘ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´,\n",
    "  - groups=1 ì—ì„œëŠ” ëª¨ë“  ì…ë ¥ì´ ëª¨ë“  ì¶œë ¥ìœ¼ë¡œ ì»¨ë³¼ë£¨ì…˜ë©ë‹ˆë‹¤.\n",
    "  - groups=2 ì—ì„œ ì—°ì‚°ì€ ë‘ ê°œì˜ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ê°€ ë‚˜ë€íˆ ìˆê³ , ê°ê° ì ˆë°˜ì˜ ì…ë ¥ ì±„ë„ì„ ë³´ê³  ì ˆë°˜ì˜ ì¶œë ¥ ì±„ë„ì„ ìƒì„±í•˜ë©°, ì´í›„ ë‘˜ ë‹¤ ì—°ê²°ë˜ëŠ” ê²ƒê³¼ ë™ì¼í•´ì§‘ë‹ˆë‹¤.\n",
    "  - groups=`in_channels` ì—ì„œëŠ” ê° ì…ë ¥ ì±„ë„ì´ ìì²´ í•„í„° ì„¸íŠ¸(í¬ê¸°ê°€ out_channels/in_channels)ë¡œ ì»¨ë³¼ë£¨ì…˜ë©ë‹ˆë‹¤.\n",
    "\n",
    "ë§¤ê°œë³€ìˆ˜ `kernel_size`, `stride`, `padding`, `dilation` ì€ ì•„ë˜ ë‘˜ ì¤‘ í•˜ë‚˜ê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "- ë‹¨ì¼ ì •ìˆ˜ `int` - ì´ ê²½ìš° ë†’ì´ì™€ ë„ˆë¹„ ì°¨ì›ì— ë™ì¼í•œ ê°’ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "- ë‘ ê°œì˜ ì •ìˆ˜ë¡œ êµ¬ì„±ëœ `tuple` - ì´ ê²½ìš° ì²« ë²ˆì§¸ ì •ìˆ˜ëŠ” ë†’ì´ ì°¨ì›ì—, ë‘ ë²ˆì§¸ ì •ìˆ˜ëŠ” ë„ˆë¹„ ì°¨ì›ì— ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "\n",
    "<br>\n",
    "\n",
    "- ì°¸ê³ \n",
    "  - groups == in_channels ë° out_channels == K * in_channels ì—ì„œ Kê°€ ì–‘ì˜ ì •ìˆ˜ì¸ ê²½ìš°, ì´ ì—°ì‚°ì„ \"ê¹Šì´ ë°©í–¥ ì»¨ë³¼ë£¨ì…˜\" (depthwise convolution) ì´ë¼ê³ ë„ í•©ë‹ˆë‹¤.\n",
    "  - ì¦‰, í¬ê¸° (ğ‘,ğ¶ğ‘–ğ‘›,ğ¿ğ‘–ğ‘›) ì…ë ¥ì—ì„œ, $(C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in})$ ì¸ìˆ˜ì™€ í•¨ê»˜ ê¹Šì´ ìŠ¹ìˆ˜ K ë¥¼ ì‚¬ìš©í•˜ëŠ” ê¹Šì´ ì»¨ë³¼ë£¨ì…˜ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- ì°¸ê³  \n",
    "  - CUDA ì¥ì¹˜ì— í…ì„œê°€ ì£¼ì–´ì§€ê³  CuDNNì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, ì´ ì—°ì‚°ìëŠ” ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ ë¹„ê²°ì •ë¡ ì  ì•Œê³ ë¦¬ì¦˜ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì´ ë°”ëŒì§í•˜ì§€ ì•Šì€ ê²½ìš°, `torch.backends.cudnn.deterministic = True` ë¡œ ì„¤ì •í•˜ì—¬ ì—°ì‚°ì„ ê²°ì •ë¡ ì ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì ì¬ì ìœ¼ë¡œ ì„±ëŠ¥ ë¹„ìš©ì´ ë°œìƒí•  ìˆ˜ ìˆìŒ). ìì„¸í•œ ë‚´ìš©ì€ \"ì¬í˜„ì„±\"ì„ ì°¸ì¡°í•˜ì„¸ìš”.\n",
    "- ì°¸ê³ \n",
    "  - `padding='valid'` ëŠ” íŒ¨ë”©ì´ ì—†ëŠ” ê²ƒê³¼ ë™ì¼í•©ë‹ˆë‹¤. `padding='same'` ëŠ” ì…ë ¥ì„ íŒ¨ë”©í•˜ì—¬ ì¶œë ¥ì´ ì…ë ¥ê³¼ ê°™ì€ ëª¨ì–‘ì„ ê°–ë„ë¡ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ëª¨ë“œëŠ” 1 ì´ì™¸ì˜ ë³´í­ ê°’ì€ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "- ì°¸ê³ \n",
    "  - ì´ ëª¨ë“ˆì€ complex32, complex64, complex128 ê³¼ ê°™ì€ ë³µì¡í•œ ë°ì´í„° ìœ í˜•ì„ ì§€ì›í•©ë‹ˆë‹¤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0035710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf7371f",
   "metadata": {},
   "source": [
    "### nn.Unfold\n",
    "https://runebook.dev/ko/docs/pytorch/generated/torch.nn.unfold\n",
    "\n",
    "torch.nn.Unfold(kernel_size, dilation=1, padding=0, stride=1)\n",
    "\n",
    "ì¼ê´„ ì²˜ë¦¬ëœ ì…ë ¥ í…ì„œì—ì„œ ìŠ¬ë¼ì´ë”© ë¡œì»¬ ë¸”ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d2cfbf",
   "metadata": {},
   "source": [
    "Consider a batched input tensor of shape (N,C,âˆ—), where N is the batch dimension, C is the channel dimension, and âˆ— represent arbitrary spatial dimensions. \n",
    "\n",
    "(N, C,âˆ—) í˜•íƒœì˜ ë°°ì¹˜ ì…ë ¥ í…ì„œë¥¼ ê³ ë ¤í•´ ë³´ê² ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ Nì€ ë°°ì¹˜ ì°¨ì›, CëŠ” ì±„ë„ ì°¨ì›ì´ë©°, âˆ—ëŠ” ì„ì˜ì˜ ê³µê°„ ì°¨ì›ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    "\n",
    "This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column (i.e., last dimension) of a 3-D output tensor of shape (N,CÃ—âˆ(kernel_size),L), where CÃ—âˆ(kernel_size) is the total number of values within each block (a block has âˆ(kernel_size) spatial locations each containing a C-channeled vector), and L is the total number of such blocks:\n",
    "\n",
    "ì´ ì—°ì‚°ì€ ì…ë ¥ì˜ ê³µê°„ ì°¨ì› ë‚´ì—ì„œ kernel_size í¬ê¸°ì˜ ê° ìŠ¬ë¼ì´ë”© ë¸”ë¡ì„, (N, CÃ—âˆ(kernel_size), L) í˜•íƒœì˜ 3ì°¨ì› ì¶œë ¥ í…ì„œì˜ ì—´(ì¦‰, ë§ˆì§€ë§‰ ì°¨ì›)ë¡œ í‰íƒ„í™”í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ CÃ—âˆ(kernel_size)ëŠ” ê° ë¸”ë¡ ë‚´ ê°’ì˜ ì´ ê°œìˆ˜ì…ë‹ˆë‹¤. (ë¸”ë¡ì€ âˆ(kernel_size)ê°œì˜ ê³µê°„ ìœ„ì¹˜ë¥¼ ê°€ì§€ë©°, ê° ê³µê°„ ìœ„ì¹˜ëŠ” Cì±„ë„ ë²¡í„°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤). Lì€ ë‹¤ìŒê³¼ ê°™ì€ ë¸”ë¡ì˜ ì´ ê°œìˆ˜ì…ë‹ˆë‹¤.\n",
    "\n",
    "$$ L = \\prod_d \\left\\lfloor\\frac{\\text{spatial\\_size}[d] + 2 \\times \\text{padding}[d] %\n",
    "    - \\text{dilation}[d] \\times (\\text{kernel\\_size}[d] - 1) - 1}{\\text{stride}[d]} + 1\\right\\rfloor $$\n",
    "\n",
    "where spatial_size is formed by the spatial dimensions of `input` (âˆ— above), and d is over all spatial dimensions. <br>\n",
    "ì—¬ê¸°ì„œ spatial_sizeëŠ” ì…ë ¥ì˜ ê³µê°„ ì°¨ì›(ìœ„ ì„¤ëª…ì˜ âˆ—)ìœ¼ë¡œ êµ¬ì„±ë˜ê³ , dëŠ” ëª¨ë“  ê³µê°„ ì°¨ì›ì…ë‹ˆë‹¤.\n",
    "\n",
    "Therefore, indexing output at the last dimension (column dimension) gives all values within a certain block. <br>\n",
    "ë”°ë¼ì„œ ë§ˆì§€ë§‰ ì°¨ì›(ì—´ ì°¨ì›)ì—ì„œ ì¶œë ¥ì„ ì¸ë±ì‹±í•˜ë©´ íŠ¹ì • ë¸”ë¡ ë‚´ì˜ ëª¨ë“  ê°’ì´ ë°˜í™˜ë©ë‹ˆë‹¤.\n",
    "\n",
    "The padding, stride and dilation arguments specify how the sliding blocks are retrieved. <br>\n",
    "padding, stride ë° dilation ì¸ìˆ˜ëŠ” ìŠ¬ë¼ì´ë”© ë¸”ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë°©ë²•ì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "- stride controls the stride for the sliding blocks.\n",
    "- padding controls the amount of implicit zero-paddings on both sides for padding number of points for each dimension before reshaping.\n",
    "- dilation controls the spacing between the kernel points; also known as the Ã  trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does.\n",
    "- \n",
    "- strideëŠ” ìŠ¬ë¼ì´ë”© ë¸”ë¡ì˜ strideë¥¼ ì œì–´í•©ë‹ˆë‹¤.\n",
    "- paddingì€ ê° ì°¨ì›ì˜ ì  ê°œìˆ˜ë¥¼ íŒ¨ë”©í•˜ê¸° ìœ„í•´ ì–‘ìª½ì— ì•”ë¬µì ì¸ 0 íŒ¨ë”©ì„ ì ìš©í•˜ëŠ” ì–‘ì„ ì œì–´í•©ë‹ˆë‹¤.\n",
    "- dilationì€ ì»¤ë„ ì  ì‚¬ì´ì˜ ê°„ê²©ì„ ì œì–´í•©ë‹ˆë‹¤. Ã  trous ì•Œê³ ë¦¬ì¦˜ì´ë¼ê³ ë„ í•©ë‹ˆë‹¤. ì„¤ëª…í•˜ê¸°ëŠ” ì–´ë µì§€ë§Œ, ì´ ë§í¬ì—ì„œ dilationì˜ ê¸°ëŠ¥ì„ ì‹œê°ì ìœ¼ë¡œ ì˜ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d05a7a",
   "metadata": {},
   "source": [
    "\n",
    "## Pooling layers\n",
    "\n",
    "Padding Layers\n",
    "\n",
    "Non-linear Activations (weighted sum, nonlinearity)\n",
    "\n",
    "Non-linear Activations (other)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e27dc6",
   "metadata": {},
   "source": [
    "\n",
    "## Normalization Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1fe99a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### nn.BatchNorm1d\n",
    "\n",
    "Applies Batch Normalization over a 2D or 3D input.\n",
    "\n",
    "### nn.BatchNorm2d\n",
    "\n",
    "Applies Batch Normalization over a 4D input.\n",
    "\n",
    "### nn.BatchNorm3d\n",
    "\n",
    "Applies Batch Normalization over a 5D input.\n",
    "\n",
    "### nn.LazyBatchNorm1d\n",
    "\n",
    "A torch.nn.BatchNorm1d module with lazy initialization.\n",
    "\n",
    "### nn.LazyBatchNorm2d\n",
    "\n",
    "A torch.nn.BatchNorm2d module with lazy initialization.\n",
    "\n",
    "### nn.LazyBatchNorm3d\n",
    "\n",
    "A torch.nn.BatchNorm3d module with lazy initialization.\n",
    "\n",
    "### nn.GroupNorm\n",
    "\n",
    "Applies Group Normalization over a mini-batch of inputs.\n",
    "\n",
    "### nn.SyncBatchNorm\n",
    "\n",
    "Applies Batch Normalization over a N-Dimensional input.\n",
    "\n",
    "### nn.InstanceNorm1d\n",
    "\n",
    "Applies Instance Normalization.\n",
    "\n",
    "### nn.InstanceNorm2d\n",
    "\n",
    "Applies Instance Normalization.\n",
    "\n",
    "### nn.InstanceNorm3d\n",
    "\n",
    "Applies Instance Normalization.\n",
    "\n",
    "### nn.LazyInstanceNorm1d\n",
    "\n",
    "A torch.nn.InstanceNorm1d module with lazy initialization of the num_features argument.\n",
    "\n",
    "### nn.LazyInstanceNorm2d\n",
    "\n",
    "A torch.nn.InstanceNorm2d module with lazy initialization of the num_features argument.\n",
    "\n",
    "### nn.LazyInstanceNorm3d\n",
    "\n",
    "A torch.nn.InstanceNorm3d module with lazy initialization of the num_features argument.\n",
    "\n",
    "### nn.LayerNorm\n",
    "- ë¯¸ë‹ˆ ë°°ì¹˜ ì…ë ¥ì— ë ˆì´ì–´ ì •ê·œí™”(Layer Normalization)ë¥¼ ì ìš©.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### nn.LocalResponseNorm\n",
    "\n",
    "Applies local response normalization over an input signal.\n",
    "\n",
    "### nn.RMSNorm\n",
    "\n",
    "Applies Root Mean Square Layer Normalization over a mini-batch of inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c640217",
   "metadata": {},
   "source": [
    "\n",
    "Recurrent Layers\n",
    "\n",
    "Transformer Layers\n",
    "\n",
    "Linear Layers\n",
    "\n",
    "Dropout Layers\n",
    "\n",
    "Sparse Layers\n",
    "\n",
    "Distance Functions\n",
    "\n",
    "Loss Functions\n",
    "\n",
    "Vision Layers\n",
    "\n",
    "Shuffle Layers\n",
    "\n",
    "DataParallel Layers (multi-GPU, distributed)\n",
    "\n",
    "Utilities\n",
    "\n",
    "Quantized Functions\n",
    "\n",
    "Lazy Modules Initialization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
