{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a11379e9",
   "metadata": {},
   "source": [
    "# 1-5. 출력 파서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21437c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain-google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f958c818",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8db39d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글 인증키 로딩\n",
    "import os, dotenv\n",
    "dotenv.load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "# GEMINI_API_KEY # 이 출력 결과도 커밋이 되지 않도록 해야 함.\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2dba543c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x117628e10>, default_metadata=(), model_kwargs={})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# 모델 로드\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,\n",
    "    # max_output_tokens=1000,\n",
    "    google_api_key=GEMINI_API_KEY\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d1b3757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "ChatGoogleGenerativeAI(\n",
      "    *,\n",
      "    name: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    cache: Union[langchain_core.caches.BaseCache, bool, NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    verbose: bool = <factory>,\n",
      "    callbacks: Union[list[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    tags: Optional[list[str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    metadata: Optional[dict[str, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    custom_get_token_ids: Optional[Callable[[str], list[int]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    callback_manager: Optional[langchain_core.callbacks.base.BaseCallbackManager] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    rate_limiter: Optional[langchain_core.rate_limiters.BaseRateLimiter] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    disable_streaming: Union[bool, Literal[\u001b[33m'tool_calling'\u001b[39m]] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    model: str,\n",
      "    api_key: Optional[pydantic.types.SecretStr] = <factory>,\n",
      "    credentials: Any = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    temperature: float = \u001b[32m0.7\u001b[39m,\n",
      "    top_p: Optional[float] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    top_k: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    max_tokens: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    n: int = \u001b[32m1\u001b[39m,\n",
      "    max_retries: int = \u001b[32m6\u001b[39m,\n",
      "    timeout: Optional[float] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    client_options: Optional[Dict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    transport: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    additional_headers: Optional[Dict[str, str]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    response_modalities: Optional[List[google.ai.generativelanguage_v1beta.types.generative_service.GenerationConfig.Modality]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    thinking_budget: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    safety_settings: Optional[Dict[google.ai.generativelanguage_v1beta.types.safety.HarmCategory, google.ai.generativelanguage_v1beta.types.safety.SafetySetting.HarmBlockThreshold]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    client: Any = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    async_client_running: Any = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    default_metadata: Sequence[Tuple[str, str]] = <factory>,\n",
      "    convert_system_message_to_human: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    cached_content: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    model_kwargs: dict[str, typing.Any] = <factory>,\n",
      ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mDocstring:\u001b[39m     \n",
      "`Google AI` chat models integration.\n",
      "\n",
      "Instantiation:\n",
      "    To use, you must have either:\n",
      "\n",
      "        1. The ``GOOGLE_API_KEY`` environment variable set with your API key, or\n",
      "        2. Pass your API key using the google_api_key kwarg\n",
      "        to the ChatGoogleGenerativeAI constructor.\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        from langchain_google_genai import ChatGoogleGenerativeAI\n",
      "\n",
      "        llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
      "        llm.invoke(\"Write me a ballad about LangChain\")\n",
      "\n",
      "Invoke:\n",
      "    .. code-block:: python\n",
      "\n",
      "        messages = [\n",
      "            (\"system\", \"Translate the user sentence to French.\"),\n",
      "            (\"human\", \"I love programming.\"),\n",
      "        ]\n",
      "        llm.invoke(messages)\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        AIMessage(\n",
      "            content=\"J'adore programmer. \\n\",\n",
      "            response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]},\n",
      "            id='run-56cecc34-2e54-4b52-a974-337e47008ad2-0',\n",
      "            usage_metadata={'input_tokens': 18, 'output_tokens': 5, 'total_tokens': 23}\n",
      "        )\n",
      "\n",
      "Stream:\n",
      "    .. code-block:: python\n",
      "\n",
      "        for chunk in llm.stream(messages):\n",
      "            print(chunk)\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        AIMessageChunk(content='J', response_metadata={'finish_reason': 'STOP', 'safety_ratings': []}, id='run-e905f4f4-58cb-4a10-a960-448a2bb649e3', usage_metadata={'input_tokens': 18, 'output_tokens': 1, 'total_tokens': 19})\n",
      "        AIMessageChunk(content=\"'adore programmer. \\n\", response_metadata={'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-e905f4f4-58cb-4a10-a960-448a2bb649e3', usage_metadata={'input_tokens': 18, 'output_tokens': 5, 'total_tokens': 23})\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        stream = llm.stream(messages)\n",
      "        full = next(stream)\n",
      "        for chunk in stream:\n",
      "            full += chunk\n",
      "        full\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        AIMessageChunk(\n",
      "            content=\"J'adore programmer. \\n\",\n",
      "            response_metadata={'finish_reason': 'STOPSTOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]},\n",
      "            id='run-3ce13a42-cd30-4ad7-a684-f1f0b37cdeec',\n",
      "            usage_metadata={'input_tokens': 36, 'output_tokens': 6, 'total_tokens': 42}\n",
      "        )\n",
      "\n",
      "Async:\n",
      "    .. code-block:: python\n",
      "\n",
      "        await llm.ainvoke(messages)\n",
      "\n",
      "        # stream:\n",
      "        # async for chunk in (await llm.astream(messages))\n",
      "\n",
      "        # batch:\n",
      "        # await llm.abatch([messages])\n",
      "\n",
      "Context Caching:\n",
      "    Context caching allows you to store and reuse content (e.g., PDFs, images) for faster processing.\n",
      "    The `cached_content` parameter accepts a cache name created via the Google Generative AI API.\n",
      "    Below are two examples: caching a single file directly and caching multiple files using `Part`.\n",
      "\n",
      "    Single File Example:\n",
      "    This caches a single file and queries it.\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        from google import genai\n",
      "        from google.genai import types\n",
      "        import time\n",
      "        from langchain_google_genai import ChatGoogleGenerativeAI\n",
      "        from langchain_core.messages import HumanMessage\n",
      "\n",
      "        client = genai.Client()\n",
      "\n",
      "        # Upload file\n",
      "        file = client.files.upload(file=\"./example_file\")\n",
      "        while file.state.name == 'PROCESSING':\n",
      "            time.sleep(2)\n",
      "            file = client.files.get(name=file.name)\n",
      "\n",
      "        # Create cache\n",
      "        model = 'models/gemini-1.5-flash-001'\n",
      "        cache = client.caches.create(\n",
      "            model=model,\n",
      "            config=types.CreateCachedContentConfig(\n",
      "                display_name='Cached Content',\n",
      "                system_instruction=(\n",
      "                    'You are an expert content analyzer, and your job is to answer '\n",
      "                    'the user's query based on the file you have access to.'\n",
      "                ),\n",
      "                contents=[file],\n",
      "                ttl=\"300s\",\n",
      "            )\n",
      "        )\n",
      "\n",
      "        # Query with LangChain\n",
      "        llm = ChatGoogleGenerativeAI(\n",
      "            model=model,\n",
      "            cached_content=cache.name,\n",
      "        )\n",
      "        message = HumanMessage(content=\"Summarize the main points of the content.\")\n",
      "        llm.invoke([message])\n",
      "\n",
      "    Multiple Files Example:\n",
      "    This caches two files using `Part` and queries them together.\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        from google import genai\n",
      "        from google.genai.types import CreateCachedContentConfig, Content, Part\n",
      "        import time\n",
      "        from langchain_google_genai import ChatGoogleGenerativeAI\n",
      "        from langchain_core.messages import HumanMessage\n",
      "\n",
      "        client = genai.Client()\n",
      "\n",
      "        # Upload files\n",
      "        file_1 = client.files.upload(file=\"./file1\")\n",
      "        while file_1.state.name == 'PROCESSING':\n",
      "            time.sleep(2)\n",
      "            file_1 = client.files.get(name=file_1.name)\n",
      "\n",
      "        file_2 = client.files.upload(file=\"./file2\")\n",
      "        while file_2.state.name == 'PROCESSING':\n",
      "            time.sleep(2)\n",
      "            file_2 = client.files.get(name=file_2.name)\n",
      "\n",
      "        # Create cache with multiple files\n",
      "        contents = [\n",
      "            Content(\n",
      "                role=\"user\",\n",
      "                parts=[\n",
      "                    Part.from_uri(file_uri=file_1.uri, mime_type=file_1.mime_type),\n",
      "                    Part.from_uri(file_uri=file_2.uri, mime_type=file_2.mime_type),\n",
      "                ],\n",
      "            )\n",
      "        ]\n",
      "        model = \"gemini-1.5-flash-001\"\n",
      "        cache = client.caches.create(\n",
      "            model=model,\n",
      "            config=CreateCachedContentConfig(\n",
      "                display_name='Cached Contents',\n",
      "                system_instruction=(\n",
      "                    'You are an expert content analyzer, and your job is to answer '\n",
      "                    'the user's query based on the files you have access to.'\n",
      "                ),\n",
      "                contents=contents,\n",
      "                ttl=\"300s\",\n",
      "            )\n",
      "        )\n",
      "\n",
      "        # Query with LangChain\n",
      "        llm = ChatGoogleGenerativeAI(\n",
      "            model=model,\n",
      "            cached_content=cache.name,\n",
      "        )\n",
      "        message = HumanMessage(content=\"Provide a summary of the key information across both files.\")\n",
      "        llm.invoke([message])\n",
      "\n",
      "Tool calling:\n",
      "    .. code-block:: python\n",
      "\n",
      "        from pydantic import BaseModel, Field\n",
      "\n",
      "\n",
      "        class GetWeather(BaseModel):\n",
      "            '''Get the current weather in a given location'''\n",
      "\n",
      "            location: str = Field(\n",
      "                ..., description=\"The city and state, e.g. San Francisco, CA\"\n",
      "            )\n",
      "\n",
      "\n",
      "        class GetPopulation(BaseModel):\n",
      "            '''Get the current population in a given location'''\n",
      "\n",
      "            location: str = Field(\n",
      "                ..., description=\"The city and state, e.g. San Francisco, CA\"\n",
      "            )\n",
      "\n",
      "\n",
      "        llm_with_tools = llm.bind_tools([GetWeather, GetPopulation])\n",
      "        ai_msg = llm_with_tools.invoke(\n",
      "            \"Which city is hotter today and which is bigger: LA or NY?\"\n",
      "        )\n",
      "        ai_msg.tool_calls\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        [{'name': 'GetWeather',\n",
      "          'args': {'location': 'Los Angeles, CA'},\n",
      "          'id': 'c186c99f-f137-4d52-947f-9e3deabba6f6'},\n",
      "         {'name': 'GetWeather',\n",
      "          'args': {'location': 'New York City, NY'},\n",
      "          'id': 'cebd4a5d-e800-4fa5-babd-4aa286af4f31'},\n",
      "         {'name': 'GetPopulation',\n",
      "          'args': {'location': 'Los Angeles, CA'},\n",
      "          'id': '4f92d897-f5e4-4d34-a3bc-93062c92591e'},\n",
      "         {'name': 'GetPopulation',\n",
      "          'args': {'location': 'New York City, NY'},\n",
      "          'id': '634582de-5186-4e4b-968b-f192f0a93678'}]\n",
      "\n",
      "Use Search with Gemini 2:\n",
      "    .. code-block:: python\n",
      "\n",
      "        from google.ai.generativelanguage_v1beta.types import Tool as GenAITool\n",
      "        llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\")\n",
      "        resp = llm.invoke(\n",
      "            \"When is the next total solar eclipse in US?\",\n",
      "            tools=[GenAITool(google_search={})],\n",
      "        )\n",
      "\n",
      "Structured output:\n",
      "    .. code-block:: python\n",
      "\n",
      "        from typing import Optional\n",
      "\n",
      "        from pydantic import BaseModel, Field\n",
      "\n",
      "\n",
      "        class Joke(BaseModel):\n",
      "            '''Joke to tell user.'''\n",
      "\n",
      "            setup: str = Field(description=\"The setup of the joke\")\n",
      "            punchline: str = Field(description=\"The punchline to the joke\")\n",
      "            rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n",
      "\n",
      "\n",
      "        structured_llm = llm.with_structured_output(Joke)\n",
      "        structured_llm.invoke(\"Tell me a joke about cats\")\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        Joke(\n",
      "            setup='Why are cats so good at video games?',\n",
      "            punchline='They have nine lives on the internet',\n",
      "            rating=None\n",
      "        )\n",
      "\n",
      "Image input:\n",
      "    .. code-block:: python\n",
      "\n",
      "        import base64\n",
      "        import httpx\n",
      "        from langchain_core.messages import HumanMessage\n",
      "\n",
      "        image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
      "        image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n",
      "        message = HumanMessage(\n",
      "            content=[\n",
      "                {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\n",
      "                {\n",
      "                    \"type\": \"image_url\",\n",
      "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n",
      "                },\n",
      "            ]\n",
      "        )\n",
      "        ai_msg = llm.invoke([message])\n",
      "        ai_msg.content\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        'The weather in this image appears to be sunny and pleasant. The sky is a bright blue with scattered white clouds, suggesting fair weather. The lush green grass and trees indicate a warm and possibly slightly breezy day. There are no signs of rain or storms.'\n",
      "\n",
      "Token usage:\n",
      "    .. code-block:: python\n",
      "\n",
      "        ai_msg = llm.invoke(messages)\n",
      "        ai_msg.usage_metadata\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        {'input_tokens': 18, 'output_tokens': 5, 'total_tokens': 23}\n",
      "\n",
      "\n",
      "Response metadata\n",
      "    .. code-block:: python\n",
      "\n",
      "        ai_msg = llm.invoke(messages)\n",
      "        ai_msg.response_metadata\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "        {\n",
      "            'prompt_feedback': {'block_reason': 0, 'safety_ratings': []},\n",
      "            'finish_reason': 'STOP',\n",
      "            'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]\n",
      "        }\n",
      "\u001b[31mInit docstring:\u001b[39m Needed for arg validation.\n",
      "\u001b[31mFile:\u001b[39m           ~/work/ai/ml_study/_venv/ml_gemini/lib/python3.13/site-packages/langchain_google_genai/chat_models.py\n",
      "\u001b[31mType:\u001b[39m           ModelMetaclass\n",
      "\u001b[31mSubclasses:\u001b[39m     "
     ]
    }
   ],
   "source": [
    "ChatGoogleGenerativeAI?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb197ad",
   "metadata": {},
   "source": [
    "## 1-5-1. CSV Parser\n",
    "\n",
    "https://wikidocs.net/231383"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9c354a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78696198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kimchi', 'Bibimbap', 'Korean BBQ', 'Bulgogi', 'Japchae']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List five {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"subject\": \"popular Korean cusine\"})\n",
    "# ['Kimchi', 'Bibimbap', 'Korean BBQ', 'Bulgogi', 'Japchae']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a602364b",
   "metadata": {},
   "source": [
    "## 1-5-2. JSON Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db55d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "# v1 warning\n",
    "# /Users/yhlee/work/ai/ml_study/_venv/ml_gemini/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3670:\n",
    "# LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally.\n",
    "# The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used.\n",
    "# Please update the code to import from Pydantic directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c4eec0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m BaseModel() -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mInit docstring:\u001b[39m\n",
      "Create a new model by parsing and validating input data from keyword arguments.\n",
      "\n",
      "Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      "\u001b[31mFile:\u001b[39m           ~/work/ai/ml_study/_venv/ml_gemini/lib/python3.13/site-packages/pydantic/v1/main.py\n",
      "\u001b[31mType:\u001b[39m           ModelMetaclass\n",
      "\u001b[31mSubclasses:\u001b[39m     BaseSettings, ExampleBase, Attachment, _AttachmentDict, ExampleCreate, AttachmentInfo, AttachmentsOperations, Attachment, ExampleUpdate, DatasetBase, ..."
     ]
    }
   ],
   "source": [
    "BaseModel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c19fecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자료구조 정의 (pydantic)\n",
    "class CusineRecipe(BaseModel):\n",
    "    name: str = Field(description=\"name of a cusine\")\n",
    "    recipe: str = Field(description=\"recipe to cook the cusine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3379adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"name of a cusine\", \"type\": \"string\"}, \"recipe\": {\"title\": \"Recipe\", \"description\": \"recipe to cook the cusine\", \"type\": \"string\"}}, \"required\": [\"name\", \"recipe\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# 출력 파서 정의\n",
    "output_parser = JsonOutputParser(pydantic_object=CusineRecipe)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59bb0e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] input_types={} partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"name of a cusine\", \"type\": \"string\"}, \"recipe\": {\"title\": \"Recipe\", \"description\": \"recipe to cook the cusine\", \"type\": \"string\"}}, \"required\": [\"name\", \"recipe\"]}\\n```'} template='Answer the user query.\\n{format_instructions}\\n{query}\\n'\n"
     ]
    }
   ],
   "source": [
    "# prompt 구성\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4371976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': '비빔밥',\n",
       " 'recipe': '비빔밥은 다양한 재료를 밥과 함께 비벼 먹는 한국의 대표적인 음식입니다.  만드는 방법은 다음과 같습니다.\\n\\n**1. 재료 준비:**\\n* 밥: 2공기\\n* 시금치: 1줌 (데쳐서 물기를 꼭 짜고, 참깨, 소금 간)\\n* 무나물: 1/2개 (채 썰어 볶음)\\n* 당근: 1/2개 (채 썰어 볶음)\\n* 고사리: 1줌 (데쳐서 물기를 꼭 짜고, 간장, 참깨 간)\\n* 버섯: 1줌 (볶음)\\n* 콩나물: 1줌 (데쳐서 물기를 꼭 짜고, 간장, 참깨 간)\\n* 소고기: 50g (불고기 양념으로 볶음)\\n* 계란: 1개 (지단으로 부침)\\n* 김: 1장 (잘게 부수기)\\n* 참깨: 약간\\n* 고추장: 1큰술\\n* 간장: 1큰술\\n* 참기름: 1큰술\\n* 다진 마늘: 1작은술\\n\\n**2. 재료 조리:** 위에 나열된 재료들을 각각 맛있게 조리합니다.  야채는 데치거나 볶아 간을 맞추고, 고기는 불고기 양념으로 볶습니다. 계란은 지단으로 부쳐줍니다.\\n\\n**3. 비빔밥 만들기:**\\n* 밥을 그릇에 담습니다.\\n* 준비된 재료들을 밥 위에 보기 좋게 올립니다.\\n* 고추장, 간장, 참기름, 다진 마늘을 넣고 잘 비벼 먹습니다.\\n* 취향에 따라 김가루와 참깨를 뿌려 먹습니다.\\n\\n**팁:**  재료는 취향에 따라 추가하거나 변경할 수 있습니다.  나물의 간은 약간 심심하게 하는 것이 좋습니다.  비빔밥 양념장은 고추장, 간장, 참기름, 다진 마늘 외에 설탕이나 식초를 추가하여 입맛에 맞게 조절할 수 있습니다.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# chain.invoke({\"query\": \"Let me know how to cook Bibimbap\"})\n",
    "chain.invoke({\"query\": \"비빔밥을 요리하는 방법을 500자 이내로 설명해줘\"})\n",
    "\n",
    "# 요리 방법 설명하는 도중이 글자가 잘린다.\n",
    "# 최대 토큰 개수 설정을 해야 할 것 같음."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_gemini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
