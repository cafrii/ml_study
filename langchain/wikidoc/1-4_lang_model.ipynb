{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2be9f23",
   "metadata": {},
   "source": [
    "# 1-4. LangChain의 언어 모델 (Model)\n",
    "\n",
    "https://wikidocs.net/231344\n",
    "\n",
    "기존의 lang_model.ipynb 는 중단하고 새로 추가. 이 노트북에서는 google genai 로 시험할 수 있는 것 만 확인하기로 함.\n",
    "\n",
    "커널은 ml_gemini 를 사용하도록 한다.\n",
    "- 커널: `ml_gemini` \n",
    "- 이 커널은 conda 가 아니라 venv 기반이다.\n",
    "- 이 리포 root 의 ignored 폴더 안에 설치되어 있다.\n",
    "- genai 튜토리얼을 진행했던 바로 그 venv 이다.\n",
    "- 모두 pip 으로만 패키지를 설치했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff583f1",
   "metadata": {},
   "source": [
    "## 1-4-3-2. Gemini (Google)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "041e939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq langchain-google-genai\n",
    "\n",
    "'''\n",
    "# 다음과 같은 경고가 표시된다.\n",
    "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\n",
    "\n",
    "요약하자면..\n",
    "지금 설치하려고 하는 langchain-google-genai 는 0.8.5 버전이다.\n",
    "이것은 google-ai-generativelanguage==0.6.15 를 필요로 한다.\n",
    "하지만 이미 google-ai-generativelanguage 0.6.18 이 설치되어 있는 상태이다.\n",
    "\n",
    "향후 조금이라도 이상한 동작을 보인다면, 바로 이 버전 불일치에 의한 것일 가능성이 매우 크다. 따라서 주의가 필요하다.\n",
    "'''\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac6a288",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain-google-genai==2.1.4\n",
    "\n",
    "# google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15,\n",
    "# but you have google-ai-generativelanguage 0.6.18 which is incompatible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b9cb48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글 인증키 로딩\n",
    "import os, dotenv\n",
    "dotenv.load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "# GEMINI_API_KEY # 이 출력 결과도 커밋이 되지 않도록 해야 함.\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "570e7f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), temperature=0.0, max_output_tokens=200, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x108508550>, default_metadata=(), model_kwargs={})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# 모델 로드\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,\n",
    "    max_output_tokens=200,\n",
    "    google_api_key=GEMINI_API_KEY\n",
    ")\n",
    "llm\n",
    "# ChatGoogleGenerativeAI(\n",
    "#     model='models/gemini-1.5-flash',\n",
    "#     google_api_key=SecretStr('**********'),\n",
    "#     temperature=0.0,\n",
    "#     max_output_tokens=200,\n",
    "#     client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x10850a210>,\n",
    "#     default_metadata=(),\n",
    "#     model_kwargs={}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16717862",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"LLM은 어떤 원리로 작동하나요? 100자 이내로 설명해주세요.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97aab7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM은 방대한 텍스트 데이터를 학습하여 단어 간의 통계적 관계를 파악합니다.  입력된 텍스트를 토대로 다음에 올 단어를 예측하는 과정을 반복하며, 문장이나 문단을 생성합니다.  즉, 확률적 언어 모델을 기반으로, 가장 가능성 높은 텍스트를 생성하는 것입니다.  딥러닝 기반의 거대한 신경망을 사용합니다.\n"
     ]
    }
   ],
   "source": [
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_gemini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
