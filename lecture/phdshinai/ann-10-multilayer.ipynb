{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Neural Network 10] 다층 퍼셉트론 구현 실습\n",
    "\n",
    "https://www.youtube.com/watch?v=fcoVlBIYD54&list=PLfGJDDf2OqlSAL9kE4FvT_rG4DH_8S4AQ&index=5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> $ (x_1) \\qquad → w_1=0.7 \\ (z_1 | h_1) $ <br>\n",
    "> $ \\quad  ↘ w_2 = 0.3 \\qquad\\qquad\\qquad ↘ w_5 = 0.55 $  <br>\n",
    "> $ \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad (z_3|o_1) $ <br>\n",
    "> $ \\quad  ↗ w_3 = 0.4 \\qquad\\qquad\\qquad ↗ w_6 = 0.45 $  <br>\n",
    "> $ (x_2) \\qquad → w_4=0.6 \\ (z_2 | h_2) $ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 항목이 1개 뿐일 때의 손실 함수 계산\n",
    "  - $ C = \\frac{1}{1} \\sum_{i}^{1} (y_1 - \\hat{y}_1)^2 $\n",
    "  - \n",
    "  - => `np.mean((y_true - y_pred) ** 2)\n",
    "`\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.w1_2_3_4 = np.random.random((self.input_size, self.hidden_size))\n",
    "        # self.w1_2_3_4 = [[1, 10], [1, 10]]\n",
    "        self.w5_6 = np.random.random((self.hidden_size, self.output_size))\n",
    "        # self.w5_6 = [[-40], [40]]  # 2x1\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X: 입력 샘플 하나. 1x2\n",
    "        # propagate inputs through the network\n",
    "        self.z1_2 = np.dot(X, self.w1_2_3_4)   # (1,2) dot (2,2) => (1,2)\n",
    "        '''\n",
    "            X = [[x1 x2]]\n",
    "            z1 = x1w1 + x2w3\n",
    "            z2 = x1w2 + x2w4\n",
    "            z1_2 = [[z1 z2]]\n",
    "                 = [[x1w1+x2w3 x1w2+x2w4]]\n",
    "                 = [[x1 x2]]@[[w1 w2]\n",
    "                              [w3 w4]]\n",
    "                    # X가 1D가 아니고 2D이므로 @로 동작함. 즉\n",
    "                    # (1,2)@(2,2) => (1,2)\n",
    "                 =    X @ w_1_2_3_4\n",
    "                 = np.dot(X, w_1_2_3_4)\n",
    "            w1_2_3_4 = [[w1 w2]\n",
    "                        [w3 w4]]\n",
    "        '''\n",
    "        self.h1_2 = self.sigmoid(self.z1_2)    # (1,2)\n",
    "        '''\n",
    "            h1 = sigmoid(z1)\n",
    "            h2 = sigmoid(z2)\n",
    "            h1_2 = [[h1 h2]]\n",
    "                 = [[sigmoid(z1) sigmoid(z2)]]\n",
    "                 = sigmoid([[z1 z2]])  # <- sigmoid is arithmetic operations.\n",
    "                 = sigmoid(z1_2)\n",
    "        '''\n",
    "        self.z3 = np.dot(self.h1_2, self.w5_6) # (1,2) dot (2,1) => (1,1)\n",
    "            # 이 경우 numpy는 차원을 축소하여 그냥 무차원 스칼라 값을 리턴한다. shape:()\n",
    "        '''\n",
    "            z3 = [[h1w5 + h2w6]]\n",
    "               = [[h1 h2]]·[[w5]\n",
    "                            [w6]]\n",
    "               = h1_2 · w5_6\n",
    "        '''\n",
    "        self.o1 = self.sigmoid(self.z3)   # 1x1\n",
    "        '''\n",
    "            o1 = sigmoid(z3)\n",
    "        '''\n",
    "        return self.o1  # 스칼라 값이 아닌 (1,1)행렬이다.\n",
    "\n",
    "    def mse_loss(self, y_true, y_pred):\n",
    "        # MSE 손실 계산\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    def backward(self, X, y, y_pred, learning_rate):\n",
    "        # 각 인자의 shape: X는 (1,2), y는 (1,1), y_pred는 (1,1)\n",
    "        # 체인룰 계산.\n",
    "        # 출력 층\n",
    "        dc_do1 = -2 * (y - y_pred)  # (1,1)\n",
    "            # MSE의 미분. 출력증이 1개 뿐이라서 간단.\n",
    "            # dc/do1 = d((y-o1)^2)/do1 = -2(y-o1)\n",
    "        do1_dz3 = y_pred * (1 - y_pred) # (1,1)\n",
    "            # sigmoid의 미분.  O'(z) = O(z)(1-O(z))\n",
    "        dz3_dw5_6 = self.h1_2  # (1,2)\n",
    "            # dz3/dw5 = d(h1w5 + h2w6)/dw5 = h1\n",
    "            # dz3/dw6 = d(h1w5 + h2w6)/dw6 = h2\n",
    "        dc_dw5_6 = dc_do1 * do1_dz3 * dz3_dw5_6  # all (1,2)\n",
    "\n",
    "        # 학습\n",
    "        self.w5_6 = self.w5_6 + learning_rate * -dc_dw5_6.T\n",
    "        '''\n",
    "            w5 = w5 - lr * dc_dw5\n",
    "            w6 = w6 - lr * dc_dw6\n",
    "                다른 x1_2, z1_2 와 달리 이 w5_6 은 1x2 가 아니고 2x1 이다.\n",
    "                일부러 이렇게 한 것으로 보임.\n",
    "            [[w5] = [[w5] - lr * [[dc_dw5]\n",
    "             [w6]]   [w6]]        [dc_dw6]]\n",
    "        '''\n",
    "\n",
    "        # 은닉층\n",
    "        dc_dw1_2_3_4 = dc_do1 * do1_dz3 * \\\n",
    "            np.dot(self.w5_6 * (self.h1_2 * (1 - self.h1_2)).T, X)\n",
    "            # 왜 여기에 .T 가 필요한가??\n",
    "        '''\n",
    "            dc_dw1_2_3_4 = [[dc_dw1 dc_dw2]\n",
    "                            [dc_dw3 dc_dw4]]\n",
    "\n",
    "            % w1~w4 까지 각 요소 별 계산\n",
    "            dc_dw1 = dc_do1 * do1_dz3 * dz3_dh1 * dh1_dz1 * dz1_dw1\n",
    "                    앞의 두개는 출력층에서 이미 구한 값이고, w1~4와 무관한 값.\n",
    "                      이를 A, B 라고 하자. 둘 다 (1,2) shape 이다.\n",
    "                    뒤의 셋은 w1~w4 에 따라 식이 다름.\n",
    "                dz3_dh1 = w5\n",
    "                dh1_dz1 = sigmoid(z1) x (1 - sigmoid(z1)) = h1(1-h1) = H1\n",
    "                    % note that h1 = sigmoid(z1)\n",
    "                dz1_dw1 = x1\n",
    "            dc_dw1 = A B w5 H1 x1\n",
    "            dc_dw2 = A B w6 H2 x1\n",
    "            dc_dw3 = A B w5 H1 x2\n",
    "            dc_dw4 = A B w6 H2 x2\n",
    "\n",
    "            !!!! 주의 !!!!\n",
    "            # 아래 식에서 보겠지만 w2 와 w3의 위치가 바뀌어 정의를 해야 이 식이 된다.\n",
    "            dc_dw1_2_3_4 = [[ dc_dw1 dc_dw3 ]\n",
    "                            [ dc_dw2 dc_dw4 ]]\n",
    "\n",
    "            dc_dw1_2_3_4 = A B [[ w5H1x1  w5H1x2 ]\n",
    "                                [ w6H2x1  w6H2x2 ]]\n",
    "                         = A B [[ w5H1 ] @[[x1 x2]]\n",
    "                                [ w6H2 ]]\n",
    "                         = A B [[w5] * [[H1]  @ [[x1 x2]]\n",
    "                                [w6]]   [H2]]\n",
    "                         = A B w5_6 H.T @ X\n",
    "        '''\n",
    "\n",
    "        # 학습\n",
    "        self.w1_2_3_4 = self.w1_2_3_4 + learning_rate * -dc_dw1_2_3_4.T\n",
    "        '''\n",
    "        w1_2_3_4 와 dc_dw1_2_3_4 의 배치를 일치시키지 않았기 때문에\n",
    "        dc_dw1_2_3_4 에 transpose를 적용하고 있음.\n",
    "\n",
    "        '''\n",
    "\n",
    "    def train(self, X_train, y_train, epochs, learning_rate):\n",
    "        # X_train의 shape은 (100,2)\n",
    "        # y_train의 shape은 (100,)\n",
    "\n",
    "        # for epoch in range(epochs):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(len(X_train)): # 입력 샘플 수 만큼\n",
    "                # forward pass\n",
    "                y_pred = self.forward([X_train[i]])\n",
    "                    # 그냥 X_train[i] 이면 (2,) 벡터일텐데, [ ]를 씌워서 (1,2) 행렬로 만들어 forward()에 전달.\n",
    "                    # 리턴값 y_pred 는 1x1 행렬\n",
    "\n",
    "                # compute and print loss\n",
    "                loss = self.mse_loss([y_train[i]], y_pred)\n",
    "\n",
    "                # backward pass\n",
    "                self.backward([X_train[i]], [y_train[i]], y_pred, learning_rate)\n",
    "                    # 전달하는 데이터의 shape: (1,2), (1,1), (1,1), ()\n",
    "\n",
    "            if np.mod(epoch, 100) == 0:\n",
    "                print('epoch=', epoch, 'loss=', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 생성\n",
    "X_train = np.random.randint(0, 2, (100,2))\n",
    "# shape 지정에 의해 100x2 랜덤 행렬 생성. 0 또는 1 로만 구성됨.\n",
    "\n",
    "# XOR 게이트. 두 값이 다르면 true -> 1\n",
    "y_train = (X_train[:,0] != X_train[:,1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [0 0]\n",
      " [1 1]]\n",
      "[1 0 0]\n",
      "<class 'numpy.ndarray'> (100,)\n"
     ]
    }
   ],
   "source": [
    "# 생성된 데이터 확인. 앞의 3개만..\n",
    "print(X_train[:3])\n",
    "print(y_train[:3])\n",
    "print(type(y_train), y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다층 퍼셉트론 선언\n",
    "mlp = MLP(input_size=2, hidden_size=2, output_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 loss= 0.27984034874769653\n",
      "epoch= 100 loss= 0.04843941681583439\n",
      "epoch= 200 loss= 0.061079411325569204\n",
      "epoch= 300 loss= 0.04668928828527995\n",
      "epoch= 400 loss= 0.028393059761331656\n",
      "epoch= 500 loss= 0.02037373157208337\n",
      "epoch= 600 loss= 0.015944864686015914\n",
      "epoch= 700 loss= 0.013140929216945305\n",
      "epoch= 800 loss= 0.01120309345011151\n",
      "epoch= 900 loss= 0.009780428760563554\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "mlp.train(X_train, y_train, epochs=1000, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: [[0 0]] [[0.05747596]]\n",
      "Predicted Output: [[1 0]] [[0.87222297]]\n",
      "Predicted Output: [[0 1]] [[0.91097168]]\n",
      "Predicted Output: [[1 1]] [[0.14241516]]\n"
     ]
    }
   ],
   "source": [
    "# 테스트 값으로 모델 값 예측\n",
    "test_input = np.array([[0, 0]])\n",
    "predicted_output = mlp.forward(test_input)\n",
    "print('Predicted Output:', test_input, predicted_output)\n",
    "\n",
    "test_input = np.array([[1, 0]])\n",
    "predicted_output = mlp.forward(test_input)\n",
    "print('Predicted Output:', test_input, predicted_output)\n",
    "\n",
    "test_input = np.array([[0, 1]])\n",
    "predicted_output = mlp.forward(test_input)\n",
    "print('Predicted Output:', test_input, predicted_output)\n",
    "\n",
    "test_input = np.array([[1, 1]])\n",
    "predicted_output = mlp.forward(test_input)\n",
    "print('Predicted Output:', test_input, predicted_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
