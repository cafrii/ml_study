{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4강. 나만의 RNN을 만들어보자 (feat.훈민정음서문)\n",
    "\n",
    "https://www.youtube.com/watch?v=cdGBloT9vDk&list=PLfGJDDf2OqlQkHqKB7uonQGeNRfUo_TMe&index=22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저자의 노트북\n",
    "\n",
    "- https://github.com/phdshinai/ANN_DL101/blob/main/DeepLearning101/VanillaRNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"\n",
    "나라의 말이 중국과 달라 문자와 서로 통하지 아니하기에 이런 까닭으로 어리석은 백성이 이르고자 할 바가 있어도 마침내 제 뜻을 능히 펴지 못할 사람이 많으니라 내가 이를 위해 가엾이 여겨 새로 스물여덟 글자를 만드노니 사람마다 하여 쉬이 익혀 날로 씀에 편안케 하고자 할 따름이니라\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- linter를 위해 먼저 선언함.\n",
    "\n",
    "# 기본적인 parameters\n",
    "epochs = 10000\n",
    "h_size = 100\n",
    "seq_len = 3\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data):\n",
    "    data = re.sub('[^가-힣]', ' ', data)\n",
    "    tokens = data.split()\n",
    "    vocab = list(set(tokens))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "    ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "    return tokens, vocab_size, word_to_ix, ix_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- linter를 위해 먼저 실행\n",
    "\n",
    "tokens, vocab_size, word_to_ix, ix_to_word = data_preprocessing(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(h_size, vocab_size):\n",
    "    U = np.random.randn(h_size, vocab_size) * 0.01\n",
    "    W = np.random.randn(h_size, h_size) * 0.01\n",
    "    V = np.random.randn(vocab_size, h_size) * 0.01\n",
    "    return U,W,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- liter\n",
    "\n",
    "U, W, V = init_weights(h_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순방향\n",
    "# 입력으로 '나라의' 가 들어오면, 타겟으로 '말이' 가 나오는 것을 기대하는 것임.\n",
    "# hprev 는 코드 상으로는 이전 단계에서 backward()의 결과로 받는 것인데.. 정확히 무슨 의미??\n",
    "\n",
    "def feedforward(inputs, targets, hprev):\n",
    "    loss = 0\n",
    "    xs, hs, ps, ys = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "\n",
    "    # seq_len 는 몇 개의 단어를 한번에 묶어서 처리할 것인지 를 의미. ???\n",
    "    # 만약 seq_len 이 3 이라면:\n",
    "    #   input 으로는 ['나라의', '말이', '중국과'] 이렇게 3개가 들어오게 된다.\n",
    "    #   그렇다면 target으로는 '달라' 가 나와야 할 것임..\n",
    "    for i in range(seq_len):\n",
    "        xs[i] = np.zeros((vocab_size, 1))\n",
    "        xs[i][inputs[i]] = 1  # 각각의 word에 대한 one hot coding\n",
    "        hs[i] = np.tanh(np.dot(U, xs[i]) + np.dot(W, hs[i - 1]))\n",
    "        ys[i] = np.dot(V, hs[i])\n",
    "        ps[i] = np.exp(ys[i]) / np.sum(np.exp(ys[i]))  # softmax계산\n",
    "        loss += -np.log(ps[i][targets[i], 0])\n",
    "    return loss, ps, hs, xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(ps, hs, xs):\n",
    "\n",
    "    # Backward propagation through time (BPTT)\n",
    "    # 처음에 모든 가중치들은 0으로 설정\n",
    "    dV = np.zeros(V.shape)\n",
    "    dW = np.zeros(W.shape)\n",
    "    dU = np.zeros(U.shape)\n",
    "\n",
    "    #-- 시간 순의 역순으로 계산을 해 나가야 한다.\n",
    "    for i in range(seq_len)[::-1]:\n",
    "        output = np.zeros((vocab_size, 1))\n",
    "        output[targets[i]] = 1\n",
    "            # output 의 one-hot encoding 이 ground-truth (정답)이다. 이걸 미리 만들어 둔다.\n",
    "\n",
    "        # 참고: loss 값 L을 직접 구할 필요가 없다. 어차피 gradient를 구하는데 L값 자체가 필요하진 않다.\n",
    "\n",
    "        # ps(확률). y_hat\n",
    "        ps[i] = ps[i] - output.reshape(-1, 1)\n",
    "        # 매번 i스텝에서 dL/dVi를 구하기\n",
    "        dV_step_i = ps[i] @ (hs[i]).T  # (y_hat - y) @ hs.T - for each step\n",
    "\n",
    "        dV = dV + dV_step_i  # dL/dVi를 다 더하기\n",
    "\n",
    "        # 각i별로 V와 W를 구하기 위해서는\n",
    "        # 먼저 공통적으로 계산되는 부분을 delta로 해서 계산해두고\n",
    "        # 그리고 시간을 거슬러 dL/dWij와 dL/dUij를 구한 뒤\n",
    "        # 각각을 합하여 dL/dW와 dL/dU를 구하고\n",
    "        # 다시 공통적으로 계산되는 delta를 업데이트\n",
    "\n",
    "        # i번째 스텝에서 공통적으로 사용될 delta\n",
    "        delta_recent = (V.T @ ps[i]) * (1 - hs[i] ** 2)\n",
    "\n",
    "        # 시간을 거슬러 올라가서 dL/dW와 dL/dU를 구하\n",
    "        # for j in range(i + 1)[::-1]:\n",
    "        for j in range(i, -1, -1): # i 부터 0 까지.\n",
    "            dW_ij = delta_recent @ hs[j - 1].T\n",
    "\n",
    "            dW = dW + dW_ij\n",
    "\n",
    "            dU_ij = delta_recent @ xs[j].reshape(1, -1)\n",
    "            dU = dU + dU_ij\n",
    "\n",
    "            # 그리고 다음번 j번째 타임에서 공통적으로 계산할 delta를 업데이트\n",
    "            delta_recent = (W.T @ delta_recent) * (1 - hs[j - 1] ** 2)\n",
    "\n",
    "        for d in [dU, dW, dV]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "    return dU, dW, dV, hs[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word 는 입력. 예: '나라의'\n",
    "# length 는 생성해 낼 출력 길이.\n",
    "#\n",
    "def predict(word, length):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[word_to_ix[word]] = 1\n",
    "    ixes = []\n",
    "    h = np.zeros((h_size,1))\n",
    "\n",
    "    for t in range(length):\n",
    "        h = np.tanh(np.dot(U, x) + np.dot(W, h))\n",
    "        y = np.dot(V, h)\n",
    "        p = np.exp(y) / np.sum(np.exp(y))    # 소프트맥스\n",
    "        ix = np.argmax(p)                    # 가장 높은 확률의 index를 리턴\n",
    "        x = np.zeros((vocab_size, 1))        # 다음번 input x를 준비\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    pred_words = ' '.join(ix_to_word[i] for i in ixes)\n",
    "    return pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본적인 parameters\n",
    "epochs = 10000\n",
    "h_size = 100\n",
    "seq_len = 3\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, vocab_size, word_to_ix, ix_to_word = data_preprocessing(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나라의',\n",
       " '말이',\n",
       " '중국과',\n",
       " '달라',\n",
       " '문자와',\n",
       " '서로',\n",
       " '통하지',\n",
       " '아니하기에',\n",
       " '이런',\n",
       " '까닭으로',\n",
       " '어리석은',\n",
       " '백성이',\n",
       " '이르고자',\n",
       " '할',\n",
       " '바가',\n",
       " '있어도',\n",
       " '마침내',\n",
       " '제',\n",
       " '뜻을',\n",
       " '능히',\n",
       " '펴지',\n",
       " '못할',\n",
       " '사람이',\n",
       " '많으니라',\n",
       " '내가',\n",
       " '이를',\n",
       " '위해',\n",
       " '가엾이',\n",
       " '여겨',\n",
       " '새로',\n",
       " '스물여덟',\n",
       " '글자를',\n",
       " '만드노니',\n",
       " '사람마다',\n",
       " '하여',\n",
       " '쉬이',\n",
       " '익혀',\n",
       " '날로',\n",
       " '씀에',\n",
       " '편안케',\n",
       " '하고자',\n",
       " '할',\n",
       " '따름이니라']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '가엾이',\n",
       " 1: '까닭으로',\n",
       " 2: '하여',\n",
       " 3: '이를',\n",
       " 4: '서로',\n",
       " 5: '스물여덟',\n",
       " 6: '글자를',\n",
       " 7: '있어도',\n",
       " 8: '능히',\n",
       " 9: '중국과',\n",
       " 10: '따름이니라',\n",
       " 11: '하고자',\n",
       " 12: '문자와',\n",
       " 13: '이런',\n",
       " 14: '달라',\n",
       " 15: '여겨',\n",
       " 16: '위해',\n",
       " 17: '나라의',\n",
       " 18: '사람마다',\n",
       " 19: '많으니라',\n",
       " 20: '날로',\n",
       " 21: '내가',\n",
       " 22: '통하지',\n",
       " 23: '아니하기에',\n",
       " 24: '못할',\n",
       " 25: '이르고자',\n",
       " 26: '마침내',\n",
       " 27: '어리석은',\n",
       " 28: '만드노니',\n",
       " 29: '씀에',\n",
       " 30: '바가',\n",
       " 31: '익혀',\n",
       " 32: '펴지',\n",
       " 33: '할',\n",
       " 34: '쉬이',\n",
       " 35: '백성이',\n",
       " 36: '제',\n",
       " 37: '사람이',\n",
       " 38: '뜻을',\n",
       " 39: '편안케',\n",
       " 40: '말이',\n",
       " 41: '새로'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, W, V = init_weights(h_size, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 11.212394506604594\n",
      "epoch 100, loss: 2.0089843922676285\n",
      "epoch 200, loss: 0.2770074302725638\n",
      "epoch 300, loss: 0.13434186810313714\n",
      "epoch 400, loss: 0.08719279150149263\n",
      "epoch 500, loss: 0.06239213672953292\n",
      "epoch 600, loss: 0.04685421144339408\n",
      "epoch 700, loss: 0.036932562015630825\n",
      "epoch 800, loss: 0.030454957073053096\n",
      "epoch 900, loss: 0.02603539851460957\n",
      "epoch 1000, loss: 0.022822474792935293\n",
      "epoch 1100, loss: 0.020315060298593562\n",
      "epoch 1200, loss: 0.018242253821771415\n",
      "epoch 1300, loss: 0.016471865882325437\n",
      "epoch 1400, loss: 0.014944055365309263\n",
      "epoch 1500, loss: 0.013624305990495383\n",
      "epoch 1600, loss: 0.012482937925428008\n",
      "epoch 1700, loss: 0.01149335588185963\n",
      "epoch 1800, loss: 0.010633884220709499\n",
      "epoch 1900, loss: 0.009887281573957319\n",
      "epoch 2000, loss: 0.009239127277243422\n",
      "epoch 2100, loss: 0.008676636529778228\n",
      "epoch 2200, loss: 0.008188169609794781\n",
      "epoch 2300, loss: 0.007763158383876019\n",
      "epoch 2400, loss: 0.007392172006623667\n",
      "epoch 2500, loss: 0.0070669668471854165\n",
      "epoch 2600, loss: 0.006780467653788179\n",
      "epoch 2700, loss: 0.006526683809254246\n",
      "epoch 2800, loss: 0.006300585120925908\n",
      "epoch 2900, loss: 0.006097962194732773\n",
      "epoch 3000, loss: 0.005915290029572083\n",
      "epoch 3100, loss: 0.0057496063334096705\n",
      "epoch 3200, loss: 0.005598409991245596\n",
      "epoch 3300, loss: 0.0054595804318231436\n",
      "epoch 3400, loss: 0.005331315545320144\n",
      "epoch 3500, loss: 0.005212084343320125\n",
      "epoch 3600, loss: 0.005100590409216791\n",
      "epoch 3700, loss: 0.0049957428133350365\n",
      "epoch 3800, loss: 0.004896632038152181\n",
      "epoch 3900, loss: 0.004802509202134259\n",
      "epoch 4000, loss: 0.004712767291812529\n",
      "epoch 4100, loss: 0.0046269232389686095\n",
      "epoch 4200, loss: 0.0045445997722284946\n",
      "epoch 4300, loss: 0.004465506358873654\n",
      "epoch 4400, loss: 0.004389419340443509\n",
      "epoch 4500, loss: 0.00431616227751681\n",
      "epoch 4600, loss: 0.004245588067572336\n",
      "epoch 4700, loss: 0.004177564280012851\n",
      "epoch 4800, loss: 0.004111962486800529\n",
      "epoch 4900, loss: 0.004048651574533108\n",
      "epoch 5000, loss: 0.0039874944702118845\n",
      "epoch 5100, loss: 0.003928347494408867\n",
      "epoch 5200, loss: 0.003871061562570835\n",
      "epoch 5300, loss: 0.003815484557607638\n",
      "epoch 5400, loss: 0.0037614643325152345\n",
      "epoch 5500, loss: 0.003708851943284128\n",
      "epoch 5600, loss: 0.003657504830394895\n",
      "epoch 5700, loss: 0.00360728974457739\n",
      "epoch 5800, loss: 0.0035580852576617373\n",
      "epoch 5900, loss: 0.003509783730519428\n",
      "epoch 6000, loss: 0.003462292640749597\n",
      "epoch 6100, loss: 0.0034155352131912055\n",
      "epoch 6200, loss: 0.003369450350566437\n",
      "epoch 6300, loss: 0.0033239919222783406\n",
      "epoch 6400, loss: 0.0032791275235896688\n",
      "epoch 6500, loss: 0.003234836854602489\n",
      "epoch 6600, loss: 0.003191109882386885\n",
      "epoch 6700, loss: 0.0031479449397971895\n",
      "epoch 6800, loss: 0.0031053468867118187\n",
      "epoch 6900, loss: 0.003063325422501512\n",
      "epoch 7000, loss: 0.003021893600884557\n",
      "epoch 7100, loss: 0.002981066566145751\n",
      "epoch 7200, loss: 0.002940860506267432\n",
      "epoch 7300, loss: 0.002901291804379349\n",
      "epoch 7400, loss: 0.0028623763634956596\n",
      "epoch 7500, loss: 0.0028241290785860585\n",
      "epoch 7600, loss: 0.0027865634323874884\n",
      "epoch 7700, loss: 0.0027496911950591553\n",
      "epoch 7800, loss: 0.00271352221163444\n",
      "epoch 7900, loss: 0.0026780642644481938\n",
      "epoch 8000, loss: 0.0026433230001106844\n",
      "epoch 8100, loss: 0.0026093019121300872\n",
      "epoch 8200, loss: 0.002576002371190895\n",
      "epoch 8300, loss: 0.002543423695489861\n",
      "epoch 8400, loss: 0.0025115632538254017\n",
      "epoch 8500, loss: 0.0024804165943031055\n",
      "epoch 8600, loss: 0.002449977591827417\n",
      "epoch 8700, loss: 0.0024202386079773333\n",
      "epoch 8800, loss: 0.0023911906574538557\n",
      "epoch 8900, loss: 0.002362823575969752\n",
      "epoch 9000, loss: 0.002335126185272724\n",
      "epoch 9100, loss: 0.002308086451783651\n",
      "epoch 9200, loss: 0.0022816916361615433\n",
      "epoch 9300, loss: 0.002255928431864237\n",
      "epoch 9400, loss: 0.002230783091451319\n",
      "epoch 9500, loss: 0.0022062415399929664\n",
      "epoch 9600, loss: 0.002182289475413733\n",
      "epoch 9700, loss: 0.002158912456031946\n",
      "epoch 9800, loss: 0.0021360959758301807\n",
      "epoch 9900, loss: 0.0021138255282509948\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "hprev = np.zeros((h_size, 1))\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for p in range(len(tokens)-seq_len):\n",
    "        inputs = [word_to_ix[tok] for tok in tokens[p:p + seq_len]]\n",
    "        targets = [word_to_ix[tok] for tok in tokens[p + 1:p + seq_len + 1]]\n",
    "\n",
    "        loss, ps, hs, xs = feedforward(inputs, targets, hprev)\n",
    "\n",
    "        dU, dW, dV, hprev = backward(ps, hs, xs)\n",
    "\n",
    "        # Update weights and biases using gradient descent\n",
    "        W -= learning_rate * dW\n",
    "        U -= learning_rate * dU\n",
    "        V -= learning_rate * dV\n",
    "\n",
    "        # p += seq_len\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'epoch {epoch}, loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    try:\n",
    "        user_input = '나라의' # #input(\"input: \")\n",
    "        if user_input == 'break':\n",
    "            break\n",
    "        response = predict(user_input,40)\n",
    "        print(response)\n",
    "    except:\n",
    "        print('Uh oh try again!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_macos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
