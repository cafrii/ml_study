

# [Paper Review] Titans는 Transformers를 뛰어넘을 수 있을까?

https://www.youtube.com/watch?v=4pNugYLtECM

노션 공개 링크: https://eight-cicada-d69.notion.site/PPT-Titans-Transformers-231a821bbd7280db9a84ce02ad6df0ab


(00:00)

네, 안녕하십니까. DSB 연구실에서 공부하고 있는 고려대학교 산업 경영공학과 천재원 석사 과정입니다. 오늘은 어떤 논문에 대한 소개를 드리는 어 세미나라기보다는이 연구 트렌드를 설명해 드리는 세미나라고 생각해 주시면 좋을 것 같고요. 그래서 제목은 theory of everything about sequence moding with 딥러닝 모델 라고 지어봤습니다.

그래서 뭐 딥러닝을 가지고 시퀀스 모델링 혹은 랭귀지 모델링을 함에 있어서의 뭐 다양한 리서치 필드들 뭐 어텐션 랭귀지 모델이 가지고 있는 메모리 혹은 요즘 되게 유행하는 테스트 타임 러닝이나 아키텍처 자체 트랜스포머나 뭐 LSTM과 같은 RNN 아키텍처와의 차이 뭐 요런 다양한 서브 필드들에 대한 내용들을 조금씩 다뤄 보고요. 그것들이 어떻게 이 시퀀스 모델링이라는 토픽 하에서 뭉쳐질 수 있는지, theory of everything 같은 것이 구축될 수 있을지, 뭐 요런 것에 대해서 좀 다뤄 보도록 하겠습니다. 사실 이 분야 자체가 내용이 워낙 방대하다 보니까 오늘은 V1 버전이라고 생각해 주시면 좋을 거 같고요. 그중에서도 오늘 저희는 여기에서 되게 중요한 타이탄스라는 모델을 좀 중심으로 알아보는 것까지 진행이 될 예정이다 라고 생각을 해 주시면 좋을 것 같습니다.

## TOC


(01:18)

그래서 오늘의 목적은 이 타이탄스라는 모델에 대해서 잘 이해하기 위해서 선행 지식인 리니어어텐션과 테스트 타임 트레이닝에 대해서 좀 알아보도록 하겠습니다. 그 전에 가볍게 오늘 세미나의 모티베이션부터 좀 소개를 드리자면요.


(01:36)

그래서 이 시퀀스 모델링이라는 어떤 큰 주제 이전에 이 타이탄스라는 모델 혹은 논문에 먼저 눈이 가신 분들도 계실 것 같습니다. 사실 되게 큰 이슈가 있었던 모델이죠. 이 타인탄스가 정말로 트랜스포머 2.0이냐라는 질문 자체가 굉장히 아이코닉 했던 질문이었다라고 생각이 들고요.

그리고 뭐 혹자들은 심지어 지금 이미 타이탄스가 구글에서 나온 어떤 모델이자 논문 이름이기 때문에 제미나이에 이미 들어가서 뭐 AB 테스트를 하고 있느니 뭐 이런 얘기도 많이 심심찮게 들을 수가 있었던 거 같습니다.

(02:16)

당장 여기에 넣지 못했지만 유튜브만 들어가도 구글 타이탄 이라고만 쳐도 유튜브 영상들이 되게 많죠. 트랜스포머는 곧 뭐 망하고 엔비디아 주가도 폭락할 수도 있다. 뭐 요런 얘기들도 많은데 어쨌든 그렇게 핫 포테이토였다라는 말씀을 전해 드리고 싶고요.

간단하게 구글 타이탄이라는 모델은 트랜스포머처럼 오토리그레시브하게 랭귀지를 제너레이팅을 할 수 있는 시퀀스 모델링, 그러니까 랭귀지 모델링을 할 수 있는 모델이다 라고 설명드릴 수가 있겠습니다. 자세한 부분은 뒤에 가서 확인을 말씀을 드릴 수가 있겠고요.


(2:45)

그런데 이 타이탄스라는 논문의 제목을 보면 이 “learn to memorized at test time”이라는 얘기 자체가 조금 보는 사람으로 하여금 좀 혹 하게 만드는 것이 있다라고 생각을 합니다.

이 딥러닝을 연구하는 사람들이라면 모두 다 공감할, 테스트 타임에 강건하게 작동 못 한다 라는 것이 어떻게 보면 딥러닝 모델의 숙명이죠. 근데 이런 모델을 테스트 타임에도 뭔가 러닝을 한다라고 적혀 있는 거 자체가 되게 매력적으로 보였을 것 같습니다.

그런데 확인해 봐야 될 점은 이 밑에 있는 표를 보시면 제가 이 테스트타임 트레이닝이라는 어떤 키워드와 이 리니어어텐션이라는 키워드를 가지고 좀 최근에 발표됐던 굵직굵직한 논문들을 제 나름대로 모아 두어본 표 인데요. 하나 확인할 수 있는 것은 이 테스트타임 트레이닝이라는 이 연구 분야 자체가 이 타이탄스 하나가 빡 띄운 그런 거라기보다는, 이 리서치 필드가 있었고 리서치 필드가 계속해서 발전 중이었고, 그리고 그중에 하나인 타이탄이 이번에 유명해진 거구나, 뭐 구글에서 나왔으니 당연히 그렇게 생각을 할 수 있겠죠.

(03:51)

그리고 그거보다 더 중요한게 저기 2023년에 발표된 논문을 보시면 learning to learn at test time 이라는 되게 이름이 이름까지 뭐 거의 똑같은 논문이 저기 있는 걸 확인을 할 수가 있고요. 그래서 뭐 어쨌든 저 논문도 되게 중요하게 봐야겠구나라는 생각을 할 수가 있죠.

하나 더 말씀드리고 싶은 것은 어 완전히 독립적일 수도 있겠습니다만 저 리니어 어텐션이라는 분야도 뭐 예전부터 그랬지만이 테스트 타임 트레이닝이라는 연구와 함께 최근까지도 계속 발전을 해 온 것을 확인을 할 수 있습니다. 이게 좀 의야하실 수도 있을 것 같아요.

(04:28)

이 어텐션을 리니얼라이즈 하는 것과 테스트 타임에 트레이닝을 하는게 어떤 상관이지라는 의문이 드실 수가 있을 것 같습니다. 제가 앞에서 theory of everything 이라는 얘기를 했던 이유가 이 리니어어텐션뿐만이 아니라 정말 상관없어 보이는 다양한 연구 분야들이이 시퀀스 모델링 혹 랭귀지 모델링 분야에서 서로 이렇게 레퍼런싱을 하면서 그들 사이에 연결 고리를 찾고, 그래서 본인 연구 분야에서 막혀 있었던 부분을 다른 연구분야에서 가서 가져와서 브레이크스루를 하거나 뭐 이 모든 것을 합쳐서 설명을 할 수 있는 theory of everything과 같은 논문 혹은 theory of everything과 같은 분석들이 나오고 있는 추세다 라는 걸 말씀드리기 위해서, 제 연구 세미나 제목도 사실은 theor of everything으로 지었다 라고 말씀드릴 수가 있겠습니다.

(05:15)

그래서 오른쪽 위에 있는 것처럼 결국 타이탄는 쉽게 말씀드리면은 메모리 모듈을 가지고 있는 RNN이다 라고 간단하게 설명을 드릴 수가 있는데, 그 사이에 뛰어 있는 여러 가지 다양한 곁다리긴 곁다리지만 되게 중요한 다른 개념들이 많이 등장을 해서 제대로 이해하기가 정말 어려운 뭐 그런 상황이다라는 걸 말씀드릴 수가 있을 것 같습니다.

(5:36)

그래서 일단 먼저 이 리니어어텐션에 대해서 알아보도록 하겠습니다.

## 리니어 어텐션


(5:41)

결국 이 리니어어텐션이라 함은 저희가 앞에 앞에 페이지에서도 보실 수 있겠지만 이 모든 쿼리가 모든 키에 대해서 어텐드를 함으로써 발생하는 빅오(N제곱)의 컴플렉시티를 리니어하게 바꿔서 좀 더 효율적으로 가져가겠다 라고 하는 것에 대한 연구이죠. 그래서 결국 뒤에 보시면 알겠지만 이 리니어 어텐션의 경우엔 결국 RNN으로 표현될 수가 있고요.

(06:06)

그래서 크게 봤을 때 결국에는 어텐션의 RNN화 라고 볼 수 있는데, 사실 이게 뭐 요즘 O(n제곱)의 컴플렉시티가 너무 크다라는 건 저희가 항상 계속 들어왔지만 흔히 쓰는 그 소프트맥스 어텐션을 계속 쓰고 있고 그럼 부담에도 불구하고 이걸 쓰는 이유는 사실 RNN이 어텐션보다 안 좋기 때문이겠죠. 성능적으로.

그럼 이거를 저희가 어텐션이 왜 RNN보다 좋을 수밖에 없냐 라고 물어본다면 저희는 보통 이렇게 대답할 수가 있을 것입니다. 아래쪽에 있는 박스에 적혀 있는 것처럼, 어텐션의 경우에는 만약에 지금 시점이 T라면 T 이전까지 발생한 모든 정보를 이렇게 저장을 해 두고 그때마다 필요한만큼 획득을 해서 사용을 할 수 있는 반면에, RNN에는 저기 보시는 것처럼 T시점에 사용할 수 있는 정보는 하나의 고정된 크기 압축되어 있는 컨텍스트 벡터를 통해서만 뭔가를 작업을 할 수 있다 라고 보여지기 때문에 그렇습니다.

(07:11)

그리고 이런 모델 구조 자체가 시퀀스 모델링에 큰 영향을 끼치는 이유는 이 시퀀스라는 것의 특징을 생각을 해 보면, 시퀀스는 어떤 시간 순서의 나열이 있는 자료 구조이고 그런데 중요한 건 저희는 현재 시점 t 라는 시점의 정보가 미래의 시점, 뒤의 시점에서 필요할지 필요하지 않을지 미리 알 수 없다라는 점에 있습니다.

뭐 예를 들면, 지금 그림에서 뭐 예를 들면 앞에이 다섯 토큰이 어떤 뉴스 기사라고 생각을 어 해 보겠습니다. 이때 만약에 지금 여섯 번째 토큰으로 뉴스 기사를 요약을 해 달라라는 어떤 정보가 비유적으로 들어왔다라고 생각을 해 보면, 어텐션의 경우에는 앞에 있는 뉴스 기사 내용들 중에 뭐 육하원칙에 따라서 되게 중요한 정보들만 가져올 수가 있겠죠.

(07:56)

그것과 반대로 이 뉴스 기사를 교정해 달라라는 쿼리가 대신 여섯 번째 자리에 들어왔다면, 반대로 뭐 꽤 정보와는 상관없지만 지엽적인 뭐 온점이나 문장 부호에 가 잘되는지 못되는지 요런 것에 대한 정보를 가지고 올 수가 있겠죠. 하지만 RNN의 경우에는 이 여섯 번째 토큰 위치에 뭐 요약을 해 달라라는 정보가 들어오든 아니면 교정을 해 달라라는 정보가 들어오든 앞에 있는 정보는 똑같은 방법으로 압축이 되어 있을 것입니다.

그러니까 미래 시점에 어떤 정보가 필요할지 모르는 상황에서 모든 정보를 압축을 해야 한다라는 특징 때문에, 필요한만큼 저장해 두고 가져오느냐 혹은 모든 정보를 압축해 두느냐라는 것 자체가 시퀀스 모델링에 큰 영향을 끼칠 수밖에 없겠구나라고 생각을 할 수가 있는 것이죠.


(8:43)

여기서 이제 다시 한번 생각을 해 보도록 하겠습니다. 그러니까 정말로 이 어텐션이라는게 모든 정보를 저장을 해 두고 필요한만큼 가지고 오느냐라는 걸 한번 확인을 해 볼 예정인데요. 사실 수식적으로 보면 너무 당연했죠.

이렇게 저희가 뭐 자주 보는 왼쪽 아래의 그림처럼, 결국 어텐션이 잘 될 수 있는 이유는 이렇게 저 쿼리와 키의 매트릭스 멀티플리케이션이다라는 얘기로 귀결될 수가 있을 것입니다. 이 쿼리와 모든 키들 간의 점수를 구할 수가 있고 그만큼의 정보를 어그리게이트 해서 가져오겠다 라는 것이 어텐션의 어떤 철학이 되기 때문이겠죠. 근데 사실이 수식에서 저 소프트맥스를 빼도 물론 완전 저 식이 동일해지진 않겠지만 어떤 비 선형성을 차치하고 나면 저 QKV가 하는 역할로만 생각을 했을 때 어 동일하다라는 것을 생각을 해 볼 수가 있습니다.

(9:40)

즉 여전히 저 쿼리가 모든 K에 대해서 내적을 해서 어떤 유사도 점수라는 걸 구할 수가 있겠죠. 그래서 결국엔 소프트맥스를 빼더라도 어텐션의 성질은 그대로 유지가 된다라고 생각을 할 수 있다라는 뜻이고 그걸 이제 그림으로 표현을 해 보자면 요렇게 되겠죠.


(9:53)

이렇게 소프트맥스가 없는 상황에서도 T시점의 쿼리 토큰 Q_t와 전체 키 정보인 K트랜스포스가 벡터 매트릭스 멀티플리케이션을 해서 얻어지게 되는 값이 결국에는 밸류를 가지고 오는 어텐션 스코어가 된다. 그리고 이것에 따라서 모든 시점에 대한 밸류가 어텐션 스코어만큼 가지고 와진다 라는 걸 확인을 할 수가 있죠. 소프트맥스가 없어도 말이죠.

p.10


(10:25)

그런데 이렇게 소프트맥스를 없애고 나면 뭐 요런게 가능하겠죠. 이런 멀티플리케이션의 경우에는 어소셔티브한 연산이기 때문에 뭐 QK를 먼저 곱하는게 아니라 뭐 키와 밸류를 먼저 곱하더라도 위에 있는 식과 동일한 계산 결과를 가지고 올 수가 있게 될 것입니다. 근데 이렇게 되면 뭔가 이 형태가 쿼리 토큰이, 고정되어 있는 크기 (여기서는) 3 * 3으로 고정되어 있는 매트릭스와 멀티플리케이션을 해서 어 어떤 정보를 가지고 오는 것으로 생각을 할 수가 있죠. 그니까 이게 어떻게 보면 앞에서 어떤 RNN처럼이 컨텍스트 매트릭스 앞에서 컨텍스트 벡터였지만 여기서 생성되는 컨텍스트 매트릭스라는 것에 어떻게 인터랙트를 해서 인포메이션을 가지고 오는 것으로 생각을 할 수가 있습니다.

p.11


(11:09)

그리고 그 컨텍스트 매트릭스라는 것을 수식적으로 분해를 해 보면 사실 이런 식으로 뭐 첫 번째 키와 첫 번째 밸류 토큰 간의 아웃터 프로덕트를 통해서 만들어지는 저런 3x3의 첫 번째 토큰에 대한 정보 한 장이랑, 두 번째 토큰에 대한 정보 한 장, 이런 식으로 쭉쭉쭉쭉쭉 더해져서 현재 시점, 여섯 번째 시점까지 만들어지는 그런 정보들의 한 장씩을 다 더한 것과 수식적으로 동일한 것을 확인을 해 볼 수가 있습니다.

p.12


(11:40)

이렇게 컨텍스트 매트릭스가 단순히 한 장 한 장씩 생성되는 정보의 누적합으로 표현이 된다면, 저희는 이것을 이전까지 더해진 모든 정보들의 합과 이번 시점에서 정생성되는 정보 저것으로 나눠서 생각을 해 볼 수가 있고, 이것에 대한 노테이션을 뭐 S_t라는 것을 이전까지 생성되었던 모든 정보 슬라이스들에 대한 합, 그리고 H_t라는 것을 이번 타임스텝에서 생성된 한 장의 정보로 이렇게 노테이션을 해 주게 되면은 아래쪽에 보시는 그림, 아래쪽에 보시는 수식과 같은 꼴로 어떤 일종의 점화식을 세울 수가 있습니다.

(12:17)

그런데 이 점화식을 딱 보면 너무나 저희가 익숙하게 생각하는 RNN의 업데이트 룰이다 라고 생각을 할 수가 있고, 심지어는 뭐 저희가 익히 알고 있는 RNN이 가지고 있는 비선형성조차도 없는 그러니까 그냥 히든 스테이트의 업데이트 룰이 그냥 단순히 K와 V의 에디션인, 되게 단순한 어떤 원시적인 형태의 RNN인 것을 확인을 할 수가 있습니다.

p.13


(12:42)

여기까지의 내용을 정리를 해 보면 저희는 어텐션에서 소프트맥스 연산을 제거함으로써 쿼리가 이전처럼 모든 키와 모든 밸류에 대해서 어텐드를 하는 것이 아니라 하나의 고정된 크기의 컨텍스트 매트릭스에 접근 하게 함으로써 어텐션을 리니어라이즈를 할 수가 있었죠. 그리고 이렇게 리니어라이즈 된 어텐션은 RNN이다 라는 것까지도 확인을 하였습니다.

(13:07)

처음으로 돌아가서 생각을 해 보면 되게 이상한 상황인 것이죠. 소프트맥스를 제거를 하더라도 여전히 모든 키와 모든 밸류에 대한 접근 권한을 가지고 있는 것 같아 보였는데 사실 연산적으로 그냥 컴프레스 하는 것과 똑같더라라는 걸 저희가 확인을 한 것입니다. 이걸로부터 생각을 할 수 있는 것은 어 사실 그러면 RNN도 이전 시점의 정보들을 필요한만큼 가지고 오는 것이었던 것인가? 혹은 반대로 어텐션이라는 것이 사실은 필요한만큼 정보를 어그리게이팅을 해서 가져오는 것이 아닌 건가 라는 의문이 들 수가 있는 것이죠.

어 물론 이 소프트맥스가 결국에는 성능에 있어서 중요한 차이네 라고 말할 수도 있지만 정말 그렇게 말할 수 있는가를 생각을 해 보면 본질적으로이 어텐션과 RNN의 차이가 압축되지 않은 고유의 밸류를 가지고 오는 것이나 고정된 크기의 메모리로 압축된 정보를 가지고 오는 것이나 그 둘의 차이는 본질적으로 없다, 같다라는 것을 인정을 하는 것이죠.

(14:07)

그래서 이런게 좀 저희가 지금까지 배웠던 것과 많이 다르지 않나라는 걸 생각을 하면서 좀 재밌었던 부분인 거 같고요. 그래서 여기서 더 나아가서 그럼 진짜로 이런 트랜스포머 RNN의 차이가 컴프레스와 이런 어텐션의 차이가 아니라면 어딜까를 고민해 보는 것도 재밌을 것 같다라는 생각을 드릴 수가 있을 것 같습니다.

p.14


(14:26)

앞에서 뭐 수식적으로 보여 드렸지만 요렇게 보여 드리면 좀 더 직관적으로 와 닿으실 것 같아요. 확실히 앞에서 봤던 리니어어텐션을 이렇게 피규어를 해 놓고 보니까 훨씬 더 RNN스럽다라는 걸 확인을 할 수가 있죠. 그리고 저 S_t가 만들어지게 되는 연산이 저 S_t-1과 H_t의 단순한 에디션이다라는 것까지도 생각을 해 두시면 좋을 것 같습니다.

p.15


(14:50)

사실 더 중요한 것은 아래에 보시는 것처럼 어떤 t 시점의 히든 스테이트, 이걸 이제 앞으로는 메모리라고 부를 텐데, 앞에 이 리니어 어텐션의 관점에서 보면 t -1 시점의 메모리에 t 시점의 정보를 업데이트 시키는 것으로도 확인을 할 수 있지만 (이 RNN의 업데이트 룰을), 사실 이거를 조금만 바꿔서 생각을 해 보면, 메모리를 어떠한 임의의 로스에 대해서 최적화를 시키는 것으로도 동일하게 해석을 할 수가 있습니다. 어떠한 로스냐 하면은 오른쪽에 로스인데요.

(15:26)

S라는 컨텍스트 매트릭스 그러니까 메모리에 K를 넣어서 생성이 되는 어떤 값 생성되는 벡터와, 밸류 간의 내적의 음수값을 로스로 사용을 하고 있습니다. 그러니까 저 값이 커지도록 옵티마이제이션이 된다라는 것이고요.

그 이유는 사실 저러한 의미의 로스를 저희의 메모리로 미분을 해 보게 되면은 -vk가 나오게 되고 저거에 대한 그레디언트 디센트니까 더하기 vk가 되는 것을 확인을 하실 수가 있겠죠. 저 로스에 대한 업데이트가 정확하게 무엇을 하는 것인지를 생각을 해 보면, S * K와 V의 내적값을 최대화시키는 것으로 생각을 해 볼 수가 있습니다.

(16:11)

이건 다르게 말하면 사실 이전 타임스텝의 메모리에 현재 타임스텝에 키를 넣었을 때 밸류가 나오도록 밸류를 리트리브 할 수 있도록 학습하는 것으로 해석을 할 수가 있겠죠.

어, 여기서부터 약간 헷갈리실 수가 있는데 저희가 앞에서 본 것처럼 키 밸류를 계속해서 더하는 것까지는 이해를 했습니다. RNN처럼 점화식 형태로 프로파게이트를 하면서 컨텍스트의 정보를 저장을 하는 것이구나라고 이제 이해를 할 수가 있었는데, 그런데 메모리에 어떤 K를 저런 식으로 인터랙트를 시키면 K를 뱉는 방향으로 옵티마이즈가 되는 것이 이해가 잘 안 되실 수가 있을 것 같습니다. 그러니까 K를 넣었을 때 V를 리트리브를 한다? 이 과정이 이 리니어어텐션이 어텐션에서 왜 필요하지? 라는 의문이 드실 수가 있을 거 같아요.

(16:54)

일단은 뭐 왜 이렇게 옵티마이즈를 하지에 대한 질문에 대한 답은 사실 저희가 이렇게 업데이트를 일부러 하는 것은 아니고요. 어떤 인터널한 가상의 로스라고 생각을 해 주시면 될 거 같아요.

그러니까 소프트맥스가 없는 어텐션을 디컴포즈를 해 봤더니 VK가 저런 식으로 RNN처럼 더해지는 것을 확인을 할 수가 있었고 이렇게 RNN처럼 더해지는 것이 사실은 저런 SK라는 벡터와 V라는 벡터의 내적값을 최대화하는 어떤 로스에 의해서 옵티마이즈가 되는 것과 그냥 동치였구나. 저런 로스를 가정을 했을 때 저것으로 인해서 최적화가 되는 것과도 사실 어 수학적으로 동일했구나라는 것으로 해석이 된다라는 뜻이죠. 그렇다 하더라도 왜 이런 옵티마이제이션이 발견이 된 거지? 뭔가 이 뒤에 숨겨진 뜻이 있나라는 의문이 드실 수는 있을 것 같습니다.

p.16


(17:48)

어, 그럼 왜 이렇게 메모리에 키를 넣었을 때 밸류가 나오는 것이 중요한지를 생각을 해 보면, 사실 결론적으로 말씀드리면 “쿼리라는 것이 사실상 가상의 키이기 때문에 그렇다” 라고 말씀드릴 수가 있을 것 같아요.

그니까 여기 이 피규어는 이제 리니어어텐션인데 사실 여기에 소프트맥스를 씌워도 동일합니다만 설명을 위해서 요 그림으로 설명을 드리자면, 만약에 저희가 이러한 그림의 어텐션에서 쿼리로 우연히 어떠한 특정 키가 들어오는 상황을 가정을 해 보겠습니다. 그러니까 예를 들면은 다섯 번째 키가 쿼리로 들어오는 상황을 가정을 하면 사실 저희가 원하는 것은 그에 대한 밸류를 그대로 반환하는 것이 목적일 것입니다. 사실이 어텐션이라는 것이 키와 얼만큼 유사하냐를 기반으로 정보를 가지고 오는 것이라고 나이브하게 정의를 한다면 결국이 상황은 어텐션의 웨이트가 사실은 다른 거는 다 0이고 다섯 번째 키와만 정확하게 맞기 때문에 다섯 번째 밸류를 가지고 오는 것에 대한 웨이트가 1이고 나머지가 전부 다 0인 상황인 것이죠. 그러니까 결국에 어텐션을 쓸 때 기본적으로 저희가 가지고 있는 어떤 키가 주어졌을 때 그에 대한 밸류를 잘 가지고 올 수 있어야 하는 것이 이 어텐션 구조에서 너무나 당연한 대전제다라는 것을 확인을 할 수가 있습니다.

(19:14)

그러니까 이렇게 보면은 제가 쿼리가 가상의 키라고 말씀을 드렸던게, 어떻게 보면 어텐션이라는 것은 이렇게 키 스페이스, 여섯 개 토큰으로 이루어져 있는 키 스페이스가 있고 여기 어딘가에 쿼리를 던졌을 때, 그러니까 쿼리가 주어졌을 때, 최대한 이 쿼리와 가까운 비슷한 키들에 대한 밸류를 위주로 가지고 오고 나머지는 무시해야 하는 메커니즘으로 생각을 할 수 있고, 이걸 잘하기 위해서는 앞에서 강조한 것처럼 너무 당연히 키에 대한 밸류를 잘 가지고 올 수 있어야 된다라는 것을 확인을 할 수가 있습니다.

p.17


(19:48)

그래서 이것을 앞에서 봤던 이 리니어어텐션의 RNN 폼에도 적용을 해 보면 결국 저희가 키를 넣었을 때 밸류가 나오게끔 만들어야 되는 컴포넌트가 정확하게 이 S_t 메모리인 것을 확인을 할 수가 있고요. 결국에 저희가 어떤 메모리에다가 키를 넣었을 때 밸류가 나오도록 인터널한 로스가 흐르는 것, 즉 S가 어떠한 키에 대한 V든지 가지고 올 수 있도록 옵티마이즈가 되는 것은 결국에 K에 대한 V의 리트리벌을 학습을 시키는 것이고, 그 이유는 사실 어텐션이라는 구조 자체가 리트리벌이 보장이 되어야 작동을 할 수 있는 것이기 때문에 그렇구나라고 이제 이 흐름을 이해를 하실 수가 있을 것 같습니다.

p.18


(20:31)

그래서 결국에 어텐션의 스테이트가 RNN의 업데이트 룰에 의해서 업데이트되는 이 과정을, 스테이트가 리트리벌 태스크에 대해서 옵티마이즈가 되는 과정으로 해석을 할 수 있다라는 것 자체가 하나의 어떤 연결 지점이 될 수 있다라고 저는 생각을 합니다.

그래서 위의 해석, 그니까 RNN 스럽게 정보를 더해 나가는 것이, 아래에 있는 해석, 키를 넣었을 때 V를 리트리브 할 수 있도록 옵티마이제이션이 되는 것과 같더라 라는 것을 이제 알아두시고 가면 좋을 것 같고요.

p.19


(21:08)

이렇게 다르게 볼 수 있다라는 것의 장점 중 하나는 다른 필드 혹은 다른 시각으로부터 나올 수 있는 솔루션을 여기로 가지고 올 수 있다라는 점이 있는 거 같습니다. 가령 이제 리트리버 태스크의 입장에서 보면 이전 타임스텝까지 옵티마이즈가 된 메모리가 있을 때 이 메모리가 현재 시점에 기억해야 될 정보, 즉 현재 시점에 키가 들어왔을 때 밸류를 뱉어내야 되는데 얘를 잘할 수 있는 능력이 있을 수도 있음에도 불구하고 계속 꾸역꾸역 똑같은 양의 정보를 이 메모리에 넣는다라는 점이 있을 수가 있겠죠.

(21:43)

그 똑같은 정보라 함은 VtKt가 될 것이고요. 그래서 사실 저희는 요런 걸 하고 싶을 수도 있습니다. 현재 이전 타임스텝까지 구축된 이 메모리가 복원 못 하는 만큼의 밸류만을 넣고 싶을 수도 있겠죠. 그 Vt 중에서 복원하지 못하는 만큼이라 함은 수식적으로 저 노란색으로 표시해 둔 부분처럼 Vt 빼기 현재 복원할 수 있는 정보, 즉 S_t-1에 k를 넣었을 때 나오는 정보를 뺀 값이 되겠죠. 지금은 근데 지금 부호가 빼기라서 저게 순서가 바뀌어 있지만 저 순서를 바꾸면은 현재 메모리가 복원할 수 있는 부분만큼을 실제에서 Vt 에서 빼준만큼만, 이렇게 레지듀얼하게 업데이트 해 주는 식으로 이걸 해석을 할 수가 있을 것입니다.

p.20


(22:34)

그리고 이렇게 만들어진 리니어어텐션의 변종의 이름이 델타넷 인데요. 이 델타넷도 사실 마찬가지로 앞에서 본 것처럼 내제적인 로스텀이 존재를 합니다. 그 로스텀을 이렇게 살펴보면 L2놈이 로스텀이 되고요. 그러니까 메모리에 키를 넣었을 때 나오는 밸류값과 실제 밸류값의 L2놈을 줄이는 것이 여기서의 로스텀이 되는 것이죠. 그게 사실 어떻게 보면 아래에 있는 앞에서 봤던 로스, 그러니까 내적값을 키우는 것이나 지금 본 것처럼 L2놈을 줄이는 것이나 이 Sk로 만들어지는 정보와 V를 가깝게 만들고자 한다는 것에서는 목적이 크게 다르지 않은 거 같은데, 그레디언트 관점에서는 이렇게나 크게 어떤 양태가 달라지는 것이 좀 신기하게 느껴지는 지점인 것 같습니다.

p.21


(23:26)

네. 여기까지 리니어어텐션에 대해서 배운 내용을 정리를 해 보면 결국 어텐션과 RNN에는 소프트맥스를 빼면 사실상 똑같더라 라는 걸 알 수가 있었죠. 그러니까 피규어나 수식을 있는 그대로 그냥 받아들였을 때, 전자인 어텐션은 모든 정보를 받아올 것만 같고 후자는 모든 정보를 뭔가 압축할 것만 같지만 사실 뭐 어텐션이나 RNN이나 뭐 컨텍스트에 저장을 해 놓고 쓴다, 아니면 컨텍스트를 모두 다 필요만큼 가져서 쓴다라는 이런 식의 접근이 사실상 어려워졌다라고 해석을 할 수가 있겠죠. 특히 어텐션이 그렇다라는 건 받아들이기 어렵지만 사실이었습니다.

(24:11)

어 그럼 왜 소프트맥스를 했을 때 우리가 알고 있는 그런 마치 필요한 정보를 쏙쏙 뽑아오는 것처럼 보이는 가장 엄청 좋은 성능의 어텐션을 확보할 수 있는가는 이제 별개로 생각해 봐야 될 문제겠죠. 그리고 두 번째로 이 RNN 특히 리니어어텐션의 스테이트를 업데이트를 하는 식은 우리가 알고 있는 것처럼, 이전까지의 스테이트에 현재 정보를 더하는 것으로 이루어지는 것도 맞지만, 가상의 로스를 최소화하도록 옵티마이즈가 된다라는 것을 확인을 할 수가 있었습니다. 그리고 그 로스는 스테이트에 키를 넣었을 때 밸류가 리트리브 되도록 하는 로스임을 확인을 할 수가 있었죠. 그리고 세 번째로 그 리트리벌 로스의 변화가 사실상 스테이트의 정보를 어떻게 담을 것인가와 직결되는 것도 확인을 할 수가 있었습니다.

(24:58)

그리고 중요한 건 제일 밑에 있는 체크 표시처럼, 요게 어떤 가상의 로스다 라는 점입니다. 그러니까 저희의 본 목적은 어떻게 보면 시퀀스 모델링이죠. 이 시퀀스 모델링을 하기 위해서 어떤 점화식과 같은 업데이트 식을 짰는데 이게 어떻게 보면 의도치 않게 전혀 관련 없어 보이는 임플리시한 로스로 옵티마이즈를 하는 꼴과 같이 해석될 수도 있었다라는 점입니다. 그니까 결정적으로 앞에서 말씀드렸던 시퀀스 모델링의 오브젝티브 펑션을 사용한 어떤 옵티마이즈와 이 인터널 한 옵티마이즈가 다른 점은 얘는 결국 끽 해봐야 스테이트다 라고 생각을 하시면 될 거 같습니다. 그러니까 저희가 오브젝티브 펑션을 사용을 해서 훈련시키는 모델이라는 건 사실 이런 V나 K나 Q를 만들어내는 웨이트들인 뭐 Wq, Wk, Wv들이 저희가 오브젝티브 펑션을 사용을 해서 훈련시키는 모델인 것이고 얘네는 이제 저희가 옵티마이즈를 하는 대상인 거죠.

(26:01)

스테이트는 단지 이 인터널 로스가 이 과정에서 자연스럽게 그레디언트를 따라서 흘러가면서 그냥 업데이트가 되는 것이었던 것으로 해석을 할 수가 있었던 것이죠. 어떻게 보면 뭐 너무 비유적일 수도 있지만 저희 의지랑 상관없이 그렇게 됐다라고 생각을 할 수가 있겠죠. 얘는 뭐 모델이 아니니까 저희가 능동적으로 옵티마이즈를 하는 대상이 아니잖아요. 정말 그런가라고 생각을 해 보실 수도 있을 것 같네요.

## 테스트 타임 트레이닝

p.23


(26:32)

네. 이쯤 하고 넘어가서 일단 테스트 타임 트레이닝에 대해서 살펴보도록 하겠습니다. 테스트 타임 트레이닝 리서치 필드에 있는 연구들이 공유하는 리서치 퀘스천은 테스트 시점에도 모델을 업데이트시킬 수 없을까 라고 정리할 수 있을 것 같습니다. 그니까 이 분야를 어떻게 보면 제가 찾은 한에서 가장 처음 제시했던 이 “Test time training with self-supervision for generalization under distribution shifts”라는 논문에서는 이런 문장으로 시작을 합니다.

(27:05)

“지도 학습은 디스트리션 시프트, 특히나 트레이닝과 테스트 간의 디스트리뷰션 시프트에 너무나 약하다” 라고 얘기를 합니다. 그러니까 트레이닝에 대해서는 잘하는데, 훈련된 것에 대해선 잘하는데 테스트에 대해선 잘 못한다라는게 문제라면, 트레이닝 시점에 보지 못한 어떤 분포의 이미지가 테스트에도 들어왔을 때, 테스트에도 그런 훈련을 시킬 수 있는 방법이 있다면 걔에 대해서도 강건하게 작동할 수 있지 않을까라는 리서치 퀘스천이다 라고 이해를 해 주시면 될 거 같고, 그래서 사실 이 처음 시작된 논문은 어떤 이미지 클래시피케이션 태스크를 하는 모델이었다 라는 점만 생각을 해 주시면 좋을 것 같습니다.

p.24


(27:47)

구체적으로 이걸 어떻게 했냐를 들어가기 전에 이 리서치 필드에서 공유를 하고 있는 하나의 전제는, 입력 이미지 그러니까 테스트 시점에서 볼 수 있는 입력 이미지에는 학습이 완료된 모델에 추가로 줄 수 있는 클루가 포함되어 있다라는 것이 전제입니다.

뭐 너무 복잡하게 얘기를 하지만 사실 이건 프리트레인과 파인튜닝이 왜 가능한지를 생각을 해보면 너무 당연한 전제다라고 생각을 할 수가 있습니다. 예를 들면 어떤 또렷한 그림으로 프리트레인드가 된 모델에 흐릿한 그림으로 파인튜닝을 한다라고 가정을 해 보겠습니다. 그리고 이게 너무나 어떻게 보면 make sense 한 이유가, 이것의 전제가 사실은 훈련 분포에 대해서 잘 학습이 된 모델이 흐린 그림도 잘 구분할 수 있게끔 그니까 파인 튜닝이 되게끔 만들 수 있는 정보가 그 흐린 그림 그 자체에 있다라는 것입니다. 그 너무 당연한 얘기죠.

(23:38)

근데 문제는 이렇게 해결을 할 수도 있지만 (트레이닝과 테스트 타임의 디스트리뷰션 시프트를), 결론적으로 많이 요런 문제를 해결하고자 하는 많은 접근 방법들, 프리트레인 파인튜닝이나 도메인 어탭테이션 같은 것들이 그 테스트 디스트리뷰션에 대한 어떤 안티시페이션이 있어야 됩니다. 그러니까 흐린 그림이 올 것이다라는 걸 알아야, 그것에 대해서 도메인 어댑테이션을 하든 파인튜닝을 하든 할 수가 있는 것이죠. 근데 이 테스트타임 트레이닝의 목적은 테스트 타임에 오는 어떠한 이미지에 대해서도 각각에 맞춰서 실제로 온라인하게 훈련 모델이 직접 실제로 최적화가 되도록 하는 방법을 찾겠다라는데 있다라고 설명드릴 수가 있겠습니다.

p.25


(29:27)

그래서 어떻게 하는가를 확인을 해 보면요. 훈련 시점에 이 모델이 메인 태스크와 셀프슈퍼바이즈드 태스크를 동시에 학습을 하게 합니다. 그러니까 이 피규어에서, 위에 있는 셀프슈퍼바이즈드 브랜치를 지우고 아래 있는 랜덤 어그멘테이션 로테이션도 지운다면, 저희가 흔히 알고 있는 그냥 이미지 클래시피케이션을 학습시키는 것과 똑같죠. 그러니까 이미지를 넣고 모델을 타고 나가서 Bird 라는 것을 예측하도록 학습을 시키는 건데, 추가되는 것이 저 어떻게 보면 회전 예측을 하는 브랜치 내지는 헤드라는 것을 따로 둔다 라고 생각을 하시면 될 것 같습니다. 그러니까 회전을 시킨 다음에 얼만큼 회전이 됐냐에 대한 예측을 하도록 하는 브렌치를 하나 따로 두는 건데요.

(30:16)

이렇게 되게 되면 이 모델은 메인 테스크를 학습함과 동시에 어떤 라벨, 그러니까 버드라는 것과 상관없는 라벨 어그노스틱한 이미지의 어떤 내제적인 피처를 또 잘 추출을 해낼 수 있는 능력을 갖도록 동시에 학습이 된다라는 것을 확인을 할 수가 있습니다.

p.26


(30:35)

이렇게 학습된 모델은 추론 시점에 어떤게 가능하냐 하면, 이 모델이 메인 태스크를 바로 수행을 하기 전에 셀프수퍼바이즈드 태스크에 대해서 모델을 실제로 업데이트를 시킬 수가 있습니다. 그러니까 테스트 시점이기 때문에 당연히 저 픽처에 라벨 그러니까 맞춰야 되는 클래스가 무엇인지는 모르죠. 하지만 앞에서 했던 것처럼 회전 예측 태스크의 경우에는 셀프수퍼바이즈드 테스크이기 때문에 저희가 임의로 이렇게 돌려서 메인 모델과 셀프수퍼바이즈드 브랜치를 태운 다음에 여기서부터 생성되는 로스를 통해서 모델을 업데이트를 할 수가 있습니다.

(31:15)

그러니까 학습되지 않은 분포로부터 받는 입력의 특징을 실제 저 메인 브랜치와 공유를 하는 저 메인 모델까지도 학습을 시킬 수 있도록 만들 수가 있다라는 것이고요. 그러니까 결국에는 추론 시점에 메인 태스크에 도움이 되는 방향으로 모델을 업데이트 시키는게 이런 테스트 타임 트레이닝이구나라고 생각을 하실 수가 있을 것 같습니다.

p.27


(31:39)

이게 더 신기한 점은 이런 식으로 실제로 저희가 모델을 테스트 타임에 트레이닝을 한다고 했으니까 실제로 트레이닝을 하는 것처럼 여러 번 하나의 이미지를 가지고 이렇게 어그멘테이션을 해서 여러 스텝을 거쳤을 때 실제로 이렇게 입력 이미지에 대해서 메인 태스크에 대한 어큐러시가 점진적으로 정말 학습을 하는 것처럼 올라가는 것을 확인을 할 수가 있습니다. 꽤 신기했던 거 같은데요.

그니까 요걸 보면 자연스럽게 드는 생각이, 이것을 이제 시퀀스 모델링에 적용할 수는 없을까라는 생각이 들 수가 있습니다. 그러니까 이건 이미지였으니까, 좀 더 나아가자면, 어떤 시퀀스에 적용할 수 있는 적합한 셀프수퍼바이즈드 러닝 오브젝티브가 있을까에 대한 질문으로 이어지게 되겠죠.

p.28


(32:31)

이런 모티베이션에서 나온 논문이 바로 이 “learning to (learn at test time)”, 방금 전에 봤던 그 논문이죠. 좀 더 상세 설명을 드리자면, 이 RNN을 가지고 시퀀스 모델링을 할 때 테스트 타임 트레이닝을 적용을 시키는 논문이다 라고 설명드릴 수가 있겠습니다. 이 백그라운드랑 프로블럼을 보면 저희가 너무 잘 아는 내용이죠.

RNN은 이 시퀀스 모델링에 있어서 트랜스포보다 효율적이지만 히든 스테이트가 제한이 되어 있기 때문에 퍼포먼스는 낮은 것을 저희가 다 알고 있습니다. 그에 대한 솔루션으로 이렇게 테스트 타임에도 어떤 업데이트가 될 수 있도록 이 RNN이라는 모듈을 개선을 하고자 했다 라고 생각을 해 주시면 될 거 같고요. 이때 앞에서 본 것처럼 self supervised learning 태스크가 필요한데 여기서는 리컨스트럭션 이라는 셀프수퍼바이즈드 태스크를 추가를 했다 라고 생각을 해 주시면 될 것 같습니다. 그런데 뭔가 어 딱 봤을 때 프로블럼이 솔루션에 잘 안 붙으실 거 같은데요.

그러니까 뭐 성능이 잘 안 나오는데 테스트 타임에 대한 테스트타임 트레이닝에 대한 얘기가 왜 나오지?라고 생각을 하실 수도 있는데, 자연스러운 의문일 거 같고요. 일단 뒤로 좀 더 넘어가 보도록 하겠습니다.

p.29


(33:43)

그래서 뭐 시퀀스 모델링에 대한 테스트 타임 트레이닝을 한다라고 하니까 요런 마인드맵을 그려 볼 수가 있습니다. 요런 식의 모델이 구성되겠구나를 유추를 할 수가 있고요.

그러니까 여기 보시면 메인 모델의 경우에는 다음 토큰을 예측을 하고 있는 것을 확인을 할 수가 있고, 그런데 이제 셀프슈퍼바이즈드 브랜치에서는 이 모델이 현재 시점에서 생성한 어떤 히든 스테이트 뭐 피처를 리컨스트럭션을 하도록 하는 그 로스를 통해서 학습이 되는 것을 확인을 할 수가 있습니다. 그래서 로테이션과의 차이를 생각을 해 보면, 일단 요 태스크 자체는 입력 데이터가 순차적으로 주어진다라는 점에서 회전이랑 좀 다르다 라는 걸 생각을 할 수가 있습니다. 회전은, 예를 들어서 다섯 번 업데이트를 하려면 이미지를 다섯 번 이렇게 로테이션을 해서 스텝을 밟아 왔어야겠지만, 여기서는 토큰 다섯 개가 들어오면 자연스럽게 첫 번째 시점에 대해서는 “붙들”을 리컨스트럭션을 하도록, 두 번째 타임스텝에 대해서는 “수”를 리컨스트럭션을 하도록, 이렇게 애초의 시퀀스 자체가 순차적으로 나열이 되어 있는 자료 구조이기 때문에, 요런 식의 자체적으로 그 데이터 자체에 애초에 어떤 스텝이 정해져 있는 태스크 라는게 이게 좀 다른 점이다라고 말씀드릴 수가 있겠고요. 그리고 어떻게 보면 사람이 이렇게 임의적으로 조작한 신태틱한 어그멘테이션에 대해서 학습을 하는 것보다 어떤 메인 모델이 생성하는 피처를 직접적으로 리컨스트럭션을 하도록 학습이 되니까 좀 더 어떻게 보면 넥스트콘 프레딕션이라는 메인 오브젝트에 도움이 되는 태스크지 않을까라고도 생각을 할 수가 있을 것 같습니다.

(35:22)

아, 근데 뭐 어쨌든 중요한 것은, (1) 이 리컨스트럭션이라는 것도 테스트 타임에 라벨 없이 사용을 할 수 있는 셀프수퍼바이즈드 러닝 기법이다라는 점, (2) 그리고 이때 재구축을 하게 되는 대상은 현재 시점에 들어온 토큰에 대해서 모델이 생성한 피처를 재구축하게 되는 것이 셀프수퍼바이즈드 태스크이다(라는 점), (3) 그리고 메인 태스크는 현재 시점까지 들어온 정보가 있을 때 다음 토큰을 예측하는 것이 메인 태스크이다라는 점 이 이제 중요하게 찍고 가야 되는 지점입니다.


(35:57)

그런데 중요한 지점은 저희는 이제 이 테스트 타임 트레이닝이라는 것을 RNN에 넣고 싶다 라는 것이 좀 문제가 되는 상황입니다.

앞에서 본 그 선행 연구의 경우에는 테스트임 트레이닝이 진행되는 대상이 전체 모델, 그러니까 아키텍처였죠. 그래서 모델을 1부분씩 나눠서, 앞부분은 메인 모델 그리고 뒷부분은 이렇게 두 브랜치로 나눠서 셀프 수퍼바이즈드랑 메인 이렇게 두 가지 헤드를 가지도록 만들 수가 있었는데, (저희가 여기서는) 이 RNN을 개선을 하고자 하는 것이고, 이 RNN은 어떤 컴포넌트고, 되게 아토믹한 컴포넌트이고, 그렇기 때문에 더 이상 나눌 수가 없는 기본적인 단위인 거죠. 뭐 억지로 저 RNN 내부에 있는 웨이트들 중에 일부를 셀프 수퍼바이즈드 러닝을 하도록 만든다라고 하더라도 어려울 수가 있는게, 예를 들면 저 세타들 중에서 zs 그리고 zx들 그러니까 아웃풋 토큰을 만드는 파라미터들은 실제로 넥스트 토큰 프레딕션을 해야 되는 웨이트들, 웨이트 매트릭스들인데, 얘네들이 현재 토큰을 재구축하도록 학습을 시키는 것은 말이 되지 않겠죠. 그러면 이제 저 ss나 sx, 그러니까 현재 히든 스테이트를 만드는 웨이트들이 셀프수퍼바이즈드 러닝으로 업데이트가 될 수 있냐를 생각을 해보면, 쟤네는 이미 쟤네 자체가 그 피처를 만들어내는 역할인데, 피처를 만드는 애가 자기 자신이 만든 피처를 재구축을 하는 것은 너무 트리비얼한 테스크이기 때문에 더 이상 어떤이 RNN 셀 내부에 있는 웨이트들에게 추가적으로 셀프수퍼바이즈드 러닝을 지키도록 할 구석이 없다 라고 이해를 할 수가 있는 것이죠.

(37:43)

아, 그러면 뭐 다른 어떤 웨이트들을 넣어야 되나 라고 생각을 할 수도 있는데, 그 전에 뭔가 더 이상 나눌 수 없는 기본적인 단위 내에서, 그래도 이 셀프수퍼바이즈드 러닝 브랜치, 앞에서 봤던 그 브랜치의 역할을 할 수 있는 부분이 없을까를 한 번 더 생각을 해 볼 수가 있고요.


(38:02)

그 결과 저자들은 다른게 아니라 RNN 내부에서 생성되는 스테이트를 리컨스트럭션 태스크로 자기지도 학습을 할 수 있는 일종의 모델로 취급을 하고 그 스테이트를 학습을 시키겠다 라고 합니다. 그런데 여기까지 들었을 때, 그렇게 해도 되나? 라는 의문이 들죠. 왜냐면 저희가 알고 있기로는 잘 학습된 모델이 있고, 이 모델이 어떤 메인 태스크를 잘 해내기 위해서 생성하는 일종의 중간 과정, 중간 부산물 같은 것이 스테이트인데, 이 스테이트를 학습을 하게 되면 뭔가 모델이 어떻게 하는 과정에 스테이트가 막 바뀌면서 모델이 하고자 하는 그 메인 태스크를 오히려 망치는게 아닌가 요런 의문이 들 수 있을 것 같은데요.

(38:48)

논문에서 이렇게 설명을 하고 있지 않지만 저는 이렇게 설명을 하는게 좋을 것 같아요. 그러니까 모델이라는 것을 생각을 해 보면, 아래 두 가지 조건을 만족 하는 것이라고 생각이 드는데요. 일단 모델은 당연히 어떤 입력이 되는 텐서가 있고 그걸 어떠한 출력 텐서로 매핑하는 함수다 라고 생각을 할 수가 있고, 또 더 나아가서 그 모델은 저희가 설계한 특정 로스에 의해서 데이터 드리븐 한 방식으로 학습이 되는 대상이어야 합니다. 그런데 저 스테이트를 모델로 취급하려고 보니까, 일단 입력 스테이트가 있고 출력 스테이트가 있는 것 같기는 하죠.

(39:30)

그러니까 입력으로 x를 받아서 저 z라는 히든 스테이트를 만들어 내니까. 이 점은 모델이다 라고 볼 수 있을 것 같은데, 저희가 중요하게 생각하는 것은 이거죠. 저 스테이트가 특정 로스에 의해서 데이터 드리븐하게 학습이 돼도 되냐? 이 부분이 문제인 것인데, 여기서 저희가 그럼 확인해 봐야 될 것은, 이 스테이트가 특정 로스, 특히 저 리컨스트럭션 태스크로, 데이터 드리븐하게 학습이 되어도 되는 대상인지를 확인을 해 보면 되겠죠.

(40:04)


그래서 우리는 RNN 즉 리니어어텐션의 스테이트를, 이제 내부 모델의 피처를 리컨스트럭션 매너로 학습을 하는 모델로 치환을 할 예정입니다. 그래서 이 RNN, 즉 리니어 어텐션의 식을 보시면 저 빨간색 박스 안에 있는 것처럼 되어 있죠.

(40:26)

그런데 이거를 이제 여기서 발생하는 저 스테이트를 학습을 시킨다 라고 하면, 오른쪽의 식처럼 표현을 할 수가 있을 것입니다. 그러니까 모델이라는 걸 좀 더 명시를 해 주기 위해서 스테이트를 나타내던 S를 W로 바꾸고, 그 다음에 저 W를 현재 타임스텝에서 만들어지는 정보인 X을 리컨스트럭션을 하는 태스크로 이렇게 그레디트 디센트로 학습을 시키게 되면, 이게 이제 나이브 한 방식의 테스트타임 트레이닝에서 제안하는 RNN 셀이 되겠죠. 이렇게 바꿔 놓고 보니까 굉장히 익숙한 것임을 확인을 할 수가 있습니다.

(41:06)


자, 저희는 앞에서 QKV로 운용이 되는 리니어 어텐션은 RNN임을 확인을 했습니다.

그러니까 저희는 결국에 이 리니어 어텐션의 스테이트가 모델이 생성하는 어떠한 피처를 리컨스트럭션을 하도록 만들고 싶은 것인데, 지금 돌아보니 리니어 어텐션의 리트리벌 태스크, 그러니까 메모리에 키를 집어넣었을 때 밸류를 리트리브하도록 훈련이 되었던 이런 인터널 로스가, 어떻게 보면 저희가 이미 도입하고 싶었던 이 리컨스트럭션 태스크, 즉 모델이 생성하는 키를 받아서 모델이 생성하는 밸류라는 것을 리컨스트럭션을 하고자 하는 저희의 어떤 의도가 담긴 로스와 사실은 똑같다는 것을 확인을 할 수가 있죠.

(41:53)

그러니까 사실은 저희는 뭐 스테이트를 어떤 모델화를 하기 위한 이런저런 작업을 하지 않아도 이미 스테이트는 어떠한 리컨스트럭션 태스크로 학습이 되고 있는 일종의 매우 단순한 하나의 매트릭스로 되어 있는 모델과 같은 존재였다 라는 것을 여기서 확인을 할 수가 있습니다. 그리고 뭐 더 나아가서 이런 리컨스트럭션 태스크를 뭐 L2놈을 가지고 학습을 하겠다, 앞에서는 뭐 내적과 같이 단순한 것이었지만, L2놈을 가지고 학습을 한다라고 하면은 사실 뭐 그것도 왼쪽과 같은 델타넷의 형식대로 풀어내질 수가 있는 것이고요.

(42:30)


그래서 결국에 제가 말씀드리고 싶은 것은, 이 모든 것이 시퀀스 모델링의 메모리와 관련된 무언가로 귀결이 된다 라는 것을 말씀을 드리고 싶었습니다. 그러니까 결국에 이 리니어어텐션의 변종을 만들기 위한 이런 노력, 혹은 뭐 리트리버의 옵티마이제이션 앞에서 봤던 그런 것들은 사실 이 테스트 타임 트레이닝과 매우 밀접하게 연관이 되어 있고, 어떻게 연관이 되어 있냐 하면 사실은 모티베이션과 솔루션을 공유하는 형태로 연관이 되어 있다 라고 말씀을 드릴 수가 있는 것이죠. 그러니까 정리를 한번 해 보자면, 앞에서 봤던 리니어어텐션으로부터 출발했던, 리니어어텐션이라는 것은 St 라는 메모리가 제한된 공간에 어떤 정보를 어떻게 집어넣을까에 대한 연구 분야였습니다. 그러니까 너무 당연히이 St라는 메모리를 어떤 수동적으로 취급을 해서 이제 바라봤던 것이죠.

(43:26)

그래서 이런 접근이라면 이렇게 VK를 통째로 넣는게 문제니까 지금 이 St라는 메모리라는 통이 가지고 있지 않는 정보만큼만 넣거나 하는 이런 리니어어텐션 베리언트들과 같은 어떤 연구 분야들이 이런 식으로 발전을 하게 되는 것이죠. 근데 테스트 타임 트레이닝은 완전히 그 역의 관점입니다. 동일하게 이제 제한된 메모리 공간 내 시퀀스 정보를 어떻게 압축할 것인가에 대한 모티베이션이지만 RNN이나 리니어 어텐션의 성능이 그렇기 때문에 안 좋아, 그러면 이 스테이트를 모델화를 시켜보자. 어차피 “가장 완벽한 스테이트는 모델이 아니겠느냐”라는 관점에서 접근을 했다라고 생각을 해 주시면 될 것 같습니다. 그러니까 스테이트가 더 이상 수동적이지 않게 되는 거죠.

(44:14)

그래서 어떻게 보면은, (이제 저 매트릭스라는 것을) 매트릭스로 존재했던 스테이트를 실제 모델화를 시켜서 안에 이제 VK가 계속 쌓이면서 만들어졌던 자연스럽게 만들어졌던 메모리인 매트릭스를 실제로 저런 MLP 레이어로 바꾸는 것입니다. 그리고 저 MLP 레이어가 정말 K를 넣었을 때 V를 리트리브를 해 올 수 있도록, 혹은 리컨스트럭션을 할 수 있도록 모델을 학습을 시키는 것이죠. 이러한 식으로 오면 테스트 타임에도 트레이닝을 할 수가 있을 것입니다.

(44:50)

그래서 사실 어떻게 보면 테스트 타임 트레이닝이라는 네이밍이 꽤 잘못된 사례라고 생각이 되는데요. (그러니까 개인적으로는), 왜냐면 지금 제가 이 테스트 타임과 테스트 타임 트레이닝과 리니어어텐션 배리언츠를 설명을 하면서, 뭐 한쪽은 스테이트의 수동성, 뭐 한쪽은 스테이트의 어떤 능동성, 모델 같은 스테이트 뭐 이런 식으로만 설명을 드렸지 테스트 타임에 트레이닝을 하기 위해서라는 말은 제가 전혀 드리지 않았거든요. 그까 개인적으로는 이 연구 분야의 이름이 뭔가 오히려 모델-like 스테이트라고 명명되는 것이 차라리 이해하기는 더 쉬울 것 같다라고 생각이 들고요.

(45:26)

물론 이 모델-like 스테이트의 가장 큰 특징 중에 하나가 테스트 타임 트레이닝이 가능하다 라는 어떤 특징을 가지고 있다고 설명을 할 수는 있을 것 같다 요 정도로 저는 생각이 드는 거 같습니다. 결론적으로 정리를 하자면 사실 모델이라는 것을 테스트 타임에 훈련시키는게 불가능할 것 같아 보였지만 사실 리니어어텐션의 스테이트를 어떤 하나의 매트릭스로 이루어진 모델로 생각을 한다면 얘네가 단순히 정보를 들고 이동하는 수동적인 형태의 어떤 텐서가 아니라, 키가 들어왔을 때 적한 밸류를 빼울 수 있도록 정보를 저장하는 모델이다라고 어 생각을 할 수 있었다라고 어 정리가 되겠죠.

(46:08)


그래서 결국에는 리니어어텐션도 이미 저희가 하고자 하는 테스트 타임 트레이닝을 하고자 하는 모델라이크 스테이트가 있는 상태였는데 얘를 어떻게 개선을 할래? 이런 질문을 받았을 때 결국에 이 사람들이 어떻게 개선을 했냐면 실제로 그 스테이트, 매트릭스 대신에 MLP 레이어를 쓴다 라는 데 있습니다.

사실 그뿐만 아니라 이렇게 메모리를 스테이트가 아니라 모델화 취급을 하게 되면 MLP와 같은 실제 모델을 사용을 할 수 있다 뿐만이 아니라 여러 가지 저희가 최적화 기법을 쓸 수가 있습니다. 어떤 태스크로 학습을 시키는데에 있어서 저희는 되게 많은 트릭들을 이미 가지고 있죠. 그래서 앞에서 말씀드린 것처럼 다른 필드나 다른 시선으로부터 얻을 수 있는 솔루션을 요 스테이트, 모델라이크 스테이트에 적용을 할 수 있다 라는 점이 이 테스트타임 트레이닝의 어떤 스테이트의 일반화로부터 얻을 수 있는 이점이다 라고 생각을 해 주시면 될 것 같습니다.

(47:08)

가령 여기 보시면, 예를 들면 저 러너블 W0의 경우에는 저희가 이 키 밸류를 계속 쌓아가는 형식의 스테이트의 경우에는 뭐 제로 번째 스테이트라는게 존재하지 않죠.

그러니까 처음 저희가 외워야 하는 어떤 키 밸류가 들어왔을 때 그게 이제 더해지면서 첫 번째 스테이트 S1 이라는 것이, 여기서는 이제 W1 이겠죠? W1 이라는게 생기면서 그 뒤로 이것들이 누적합이 되면서 어떤 스테이트가 되는 건데, 이걸 이제 모델이라고 생각을 해 보면 키가 들어왔을 때 밸류를 내뱉는 모델이라는게 키가 들어왔을 때만 생길리라는 보장은 없으니까 그런 모델을 이미 처음부터 이니셜라이징을 해서 따로 훈련을 시킬 수도 있을 것이고, 아니면 레이어 노말라이제이션이나 레지듀얼 같이 이걸 모델로 본다면 쓸 수 있는 이런 다양한 최적화 기법들을 적용을 할 수 있다는 점이 이 어떤 큰 도약 지점 중에 하나다 라고 정리를 할 수가 있겠습니다.

(48:07)

물론 그에 따른 단점도 존재를 하겠죠. 매트릭스 형태로 존재하던 스테이트를 모델로 바꿈으로써 여러 가지 뭐 훈련의 병목 같은게 생길 수도 있고, 최적화를 시키는 거 자체가 좀 더 복잡해지고 어려워질 수도 있습니다. 뭐 그런 것으로 인해서 생기는 문제는 뒤에 타이탄에서 좀 더 다뤄 보도록 하겠고요.

(48:28)


그래서 여기까지 봤을 때 하나의 axiom이 더 추가가 된다면, “리니어 어텐션의 스테이트라는 것은 테스트타임 트레이닝의 모델의 일종이다” 라고 생각을 할 수가 있고, 특히나 저 리니어 어텐션의 스테이트는 이제 테스트타임의 모델인 경우 중에서도 단일 매트릭스인 모델을 가지고 있는 경우에 해당된다라고 설명을 드릴 수가 있겠습니다.

## Titans

(48:58)

그 다음으로 타이탄 파트에 넘어와서요. 타이탄은 어떻게 테스트타임 트레이닝에서 제한된 방법론을 개선하였는지 더불어서 학습 과정에서 이제 발생되는 추가적인 병목 현상을 어떻게 개선했는지 등을 이제 살펴보도록 하겠습니다.


(49:16)

그래서 앞서 본 “learning to learn at test time”과 타이탄스의 차이가 무엇인지 확인을 해 보면, 사실 여러가지 에드온 한 것들이 디테일하게 보면 더 있지만, 사실 핵심적으로는 메모리 학습을 좀 더 고도화를 시킨 버전이 타이탄이다 라고 생각을 해 주시면 될 것 같습니다. 앞서서 “learning to learn at test time”의 경우에는 매트릭스 형태로 생긴 스테이트를 모델처럼 생각을 해서 이것을 MLP 레이어로 바꾸거나 아니면 레이어 노말라이제이션 같은 것을 추가해서 학습을 시킬 수 있다 라고 얘기를 했었죠. 거기서 좀 더 나아가서 타이탄의 경우에는 이게 메모리가 모델이라면 이거를 학습시킬 때 저희가 많이 자주 사용하는 어떤 트릭인 로스 텀에 모멘텀을 넣자, 혹은 웨이트 디케이를 넣자, 웨이트 모멘텀과 웨이트 디케이를 넣자 라고 어프로치를 취했다고 생각을 해 주시면 될 거 같습니다. 그래서 위에 보시면 아까 기존에 봤던 그냥 learning to learn at test time의 로스 펑션이 있고요.

(50:21)

거기서 타이탄의 업데이트 룰로 어떻게 바뀌는지를 보면 저기 저 Mt가 모델이라고 생각을 해 주시면 되고, 바뀌는 부분은 그래서 결국에는 저 빨간색 부분과 파란색 부분이 추가가 되는 것 외에는 그대로인 걸 확인을 할 수가 있습니다. 그래서 저 파란색 부분이 모멘텀 텀이 되는 것이고요. 빨간색 부분이 웨이트 DK 텀이 되는 것입니다. 저 외에는 사실 큰 차이가 없는 것인데, 당연히 로스 옵티마이제이션 관점에서 보면 메모리 모듈을 학습시키는, 최적화를 시킬 로스에 대해서 좀 안정적으로 학습을 할 수 있게끔 두 텀을 추가했다 라고 생각을 할 수가 있습니다. 그 둘이 무엇인지는 잘 알고 계실 것이라고 생각을 해서 넘어가도록 하겠고요.

p.41


(51:04)

그것보다도 이 모멘텀과 웨이트 디케이 각각, 로스 옵티마이제이션 관점이 아니라 시퀀스 모델링에 사용이 되는 메모리를 업데이트하는 관점에서 봤을 때 어떠한 역할을 각각 하는지를 일단은 살펴보면 좋을 것 같습니다. 일단 저자들이 제시한 타이탄의 경우에는 이 모멘텀을 추가해서 메모리를 업데이트 하는 것은 결국엔 패스트 서프라이즈를 고려해서 메모리를 업데이트 하는 것과 동치다 라고 생각을 할 수가 있습니다.

이 서프라이즈가 무엇이냐 하면, 사실 타이탄에서는 이전 기억, 이전 정보라는 말보다는 기억해야 할 것을 이 서프라이즈라는 용어를 써서 설명을 하는데요. 왜 서프라이즈라고 하냐면 추가된 텀이 아니라 기존에 있었던 저 텀을 예로 들자면, 저건 이제 모멘터리 서프라이즈라고 이제 저자들이 얘기를 하게 되는데요. 보시면은 이전 기억 이전 메모리 모듈에 지금 정보가 들어왔을 때 얼만큼 로스를 업데이트 할 것인가에 대한 것이 기존에 썼던 저희의 그 SGD 로스에 들어가는 텀이고, 예를 들면은 저 직전 기억, 직전 메모리에 현재 어떤 정보를 넣었을 때 이미 잘 뱉는다면 이것은 놀라운 정보가 아니겠지만, 저게 만약에 놀라운 정보라면 저 로스가 되게 커질 것이고 그만큼 많은 정보를 업데이트 할 것이다 라는 의미에서 저 그레디언트 텀 자체가 서프라이즈다, 서프라이즈라는 매트릭이다 라고 설명을 하고 있습니다.

그러고 나면 이제 저 패스트 서프라이즈의 경우에는 이전 타임스텝에서 이제 모아졌던 정보들, 모아졌던 그 서프라이즈 놀라움을 의미를 하겠죠. 그니까 저건 어떻게 보면은 지금까지 어떤 정보들이 어느 방향으로 서프라이즈를 어떤 방향으로 놀라웠는지에 대한 정보들이 에타(η)만큼 이렇게 누적되어서 저장이 되어 있겠죠.

(53:08)

이게 사실은 그래서 모멘텀과 똑같지만 메모리 업데이트 관점에서 보면 이렇게 해석을 할 수 있다라는 것이고요. 그리고 추가로 보시면 저 에타 텀에 t가 붙어 있는 것을 확인할 수 있습니다. 보통 모멘텀이라고 하면 이전까지의 어떤 그레디언트를 스태틱한 스칼라값, 그러니까 고정되어 있는 0.9 혹은 0.99 만큼의 스칼라 만큼 디케이를 시키는 것으로 이제 알고 계실 수가 있는데, 여기서는 저 에타t 라는 것도 현재 정보에 디펜던트하게 구해지는 어떤 값으로 이제 설정이 되게 됩니다. 관련해서는 뒤에 수식과 함께 좀 더 설명을 드리도록 하겠고요.

p.42


(53:47)

그 다음에 웨이트 디케이가 들어가는 저 텀의 경우에는 이제 저 알파t 만큼의 메모리를 이제 지워 주는 것을 확인을 할 수 있죠. 이제 저 알파t의 경우에는 0부터 1 사이의 값이고 만약에 알파t가 0이라면 기존에 이제 뭐 업데이트 룰과 똑같겠지만 1에 가까워지면 가까워질수록 지금까지 가지고 있는 메모리를 삭제하고 새로 업데이트하는 저 St로 지금 메모리 모듈을 싹다 갈아 뭐 없는 뭐 그런 상황이 온다라고 생각을 해 주실 수가 있을 것 같습니다. 그러니까 사실은 메모리의 업데이트 관점에서 보면 기억 공간에 있는 모든 정보를 싹 다 이제 플러시를 해 버릴 수 있는 그런 기능을 추가하는 것이 이 웨이트 디케이 기능이다 라고 생각을 할 수가 있겠죠. 여기서도 확인을 할 수 있는 점이, 저 알파t, 얼마나 메모리를 디케이 시킬 것인가, 즉 얼마나 지금 모델에 있는 정보를 없애 버릴 것인가에 대한 값도 데이터 디펜던트하게 t 시점에 들어오는 값이 무엇이냐에 따라서 알파 t라는 값이 정해지는 그런 어떤 값이다 라고 생각을 해 주시면 될 것 같습니다.

p.43


(55:01)

그래서 지금까지 확인해 봤던 것을 이렇게 그래픽과 슈도 코드를 통해서 확인을 해 보자면 이렇게 되겠죠. 가장 원시적인 형태의 RNN이자 리니어어텐션의 경우에는 요런 식으로 한 스텝 한 스텝 마다 정보가 업데이트가 되었을 것입니다. 우선은 키와 밸류에 의해서 만들어진는 벡터의 아우터프로덕트인 Ht와, 이전 스텝까지의 정보가 저장이 되어 있는 스테이트인 S_t-1이 그냥 단순히 더해져서 St가 만들어지게 되고요. 저 St의 쿼리 토큰이 이제 벡터 매트릭스 멀티플리케이션을 통해서 얻어지는 값이 이제 Ot가 되어서 위로 올라가게 되는 것을 확인을 할 수가 있습니다.

p.44


(55:40)

Learning to learn a test time 레이어의 경우에는 바뀌는 것이 오직 저 Mt의 디자인이었죠. 그래서 이제는 요게 하나의 모델화가 되는 것이고, 스테이트에 단순히 더하는 식으로 업데이트를 했던 것이 이제 저건 진짜로 모델이 되어서 저 M_t-1, 이전 타임스텝에까지의 기억을 가지고 있는 메모리 모듈에 Kt라는 현재 시점의 정보를 넣어서 Vt라는 기억해야 할 타겟 정보를 리컨스트럭션 혹은 리트리브를 하는 일종의 어떤 로스를 통해서 그레디언트 디센트가 일어나게 하게 되는 식으로 메모리가 업데이트가 되게 되고요. Qt도 이제 더 이상 벡터 매트릭스 멀티플리케이션으로 저 정보를 가지고 오는 것이 아니라 Mt라는 현재 시점의 기억을 가지고 있는 메모리 모듈에 포드를 해서 Ot가 얻어지게 된다라고 생각을 해 주시면 될 것 같습니다.

p.45


(56:35)

그리고 타이탄스로 넘어오게 되면은 피규어가 좀 더 복잡해진 것을 확인을 할 수가 있죠. 일단 모멘텀 텀인 저 보라 색깔이 추가가 되는 것부터가 좀 복잡해 보이는데요. 일단 단계별로 하나씩 확인을 해 보자면, 앞에서 봤던 모멘텀과 웨이트 디케이의 어떤 스칼라 파라미터, 저 알파t와 에타t를 구할 때, 오른쪽에 있는 수도 코드에 보이시는 것처럼 어떤 펑션에 현재 입력을 통과시켜서 얼마나 플러시를 할 것인지 혹은 얼마나 모멘텀을 디케이시킬 것인지를 정하게 됩니다. 그리고 저기서 알파 펑션 에타 펑션이라고 되어 있지만, 저게 뉴럴넷, 학습되는 뉴럴넷이라고 생각을 해 주시면 되고요. 추가로 일종에 러닝 레이트 역할을 하는 세타도 세타t로 노테이트 되어 있는 것을 보시면 아시겠지만, 현재 입력이 들어오는 것을 통해서, 이제 현재 입력에 대해서 저 에타t가 디펜던트하게 구해지는 것을 확인을 하실 수가 있습니다.

(57:34)

그래서 얼마나 그레디언트 디센트를 시킬 것인가, 이 러닝 레이트도 스태틱하거나 어떤 외부에 스케줄러를 쓰는게 아니라 이런 식으로 학습이 되어지는 대상이다 라고 생각을 해 주시면 될 거 같습니다.


(57:49)

그러고 나면 이제 메모리에, 앞에서 했던 것처럼 키와 밸류를 넣고, 현재 키를 얼마나 잘 리컨스트럭션을 해야 되느냐 혹은 타이탄의 입장에서 보면은 얼마나 현재 정보가 서프라이즈한 정보이냐 에 따라서, 이제 얼마만큼 모델을 어느 방향으로 업데이트 시킬지가 이렇게 정해지게 되겠죠. 그게 이제 M_t-1의 키와 밸류가 들어가서 얻을 수 있게 되는 저 로스가 되게 되는 것이고요.


(58:21)

이 값을 그대로 이제 모델을 업데이트 시키는데 사용하는 것이 아니라 모멘텀 만큼 이제 이동된 그레디언트를 써서 업데이트를 시키겠죠. 그렇기 때문에 에타t 만큼 디케이된 이전까지의 모멘텀 정보에다가, 현재 정보까지를 추가시켜서 이만큼을 모델을 업데이트를 시키는데 사용이 되게 됩니다.


(58:43)

그래서 이 모멘텀으로 인해서 변동된 그레디언트 정보 만큼 모델이 업데이트 됨과 동시에, 이전 타임 스텝까지의 정보를 가지고 있는 메모리 모듈에 정보 또한 앞에서 구한 알파t라는 웨이트 디케이 팩터만큼 정보를 잊게 되는 상태가 되고요. 그렇게 잊혀진, 일정 정보를 잊은 이 모델에다가 모멘텀만큼의 그레디언트가 업데이트가 되게 된다 라고 설명드릴 수가 있을 것 같습니다.

p.49


(59:16)

그래서 여기까지가 성능에 대한 이야기였고요. 사실 어떻게 보면 더 중요한 얘기인데 저희가 안 한 것이 있죠. 이러한 모델을 어떻게 학습시킬 것인가에 대한 얘기도 해야 되는데, 사실 저희가 메모리 모듈의 성능을 높이겠다라는 생각으로 매트릭스였던 걸 MLP로 바꾸고 MLP에 모멘텀도 추가하고 이렇게 이렇게 하고 있는데, 이렇게 성능을 높이기 위해서 임의로 추가한 이런 요소들이 사실 훈련의 엄청난 복잡성을 야기할 수도 있습니다. 그러니까 예를 들면, 원래 매트릭스였기 때문에 단순하게 진행됐던 많은 것들이 이렇게 모델로 바꿈으로 인해서 복잡해지 않는지를 생각을 해 봐야 하는 것이고요. 그래서 이런 학습의 효율화, 더 정확하게 말씀드리자면 학습의 병렬화가 여전히 가능한가에 대해서 이제 살펴보기 위해서, 우선 이 리니어어텐션의 듀얼 폼이라는 것부터 먼저 짚고 넘어가면 좋을 것 같습니다.

(1:00:13)

리니어어텐션의 경우에는 앞서 말씀드린 것처럼 이런 패럴렐 폼, 매트릭스 멀티플리케이션으로 모든 연산을 한 번에 수행하는 이런 폼과, 리니어 폼 그러니까 고정된 컨텍스트 벡터만을 운용을 해서 연산을 할 수 있는 이런 폼, 두 가지의 폼을 이제 자유 자재로 옮겨가면서 연산을 취할 수 있다라는 장점이 있습니다. 그러니까 예를 들면, 소프트맥스 어텐션의 경우에는 위의 형식으로만 연산이 되겠죠. 그리고 뭐 다양한 RNN의 경우에는 어떤 비선형성들 때문에, 아래에 있는 리커런트 폼에서 벗어나서 위에 있는 패럴렐 폼으로 이게 병렬적으로 계산이 안 된다라는 것도 잘 알고 계실 겁니다. 그리고 이 각각의 연산에는 사실 장단점이 존재하게 되는데요.


(1:00:59)

우선 이 패럴렐 폼에서의 어떤 장점을 들자면, 다들 트랜스포머의 장점에 대해서 항상 나오는 것이기 때문에 잘 알고 계시겠지만, 시퀀스 랭스 차원에서의 병렬 연산이 가능합니다. 이것의 이유는 필수적인 선행 연산이 존재하지 않기 때문인데요. 그러니까 시퀀스 차원 연산에서 필수 선행 연산이 존재하지 않는다라는 것의 의미는, 예를 들면 저 뒤에 토큰, 100번째 토큰의 어떤 값을 계산하기 위해서 그 앞의 1번부터 99번까지의 어떤 토큰 연산의 결과를 미리 가지고 있을 필요는 없다 라는 것이죠. 이것들이 싹 다 병렬적으로 연산이 될 수 있다라는 뜻이고요.

하지만 저 마스크로 인해서 리던던트한 연산량 자체는 증가하게 됩니다. 무슨 뜻이냐 하면, 사실 Q와 K의 매트릭스 멀티플리케이션“만” 보면, 첫 번째 쿼리 토큰이 여섯 번째 키 토큰에 어텐드를 하는 값도 사실은 연산을 하게 될 것입니다. 하지만 그게 사실 연산되지 않은 것처럼 그러니까 연산의 영향을 없애 주기 위해서 저렇게 상성각행렬의 마스크를 이제 씌워서 엘리먼트 와이즈하게 곱해 주는 것을 저희는 알고 있죠. 그래서 그만큼의 연산은 사실 필요하지 않은데 진행이 되는 것이고요.

(1:02:21)

그래서 어떻게 보면 이거는 인풋 아웃풋은 굉장히 적지만, 그러니까 한 번에 연산이 되지만, 플롭스가 비교적 높은 연산이다 라고 설명드릴 수가 있겠습니다. 인풋 아웃풋이라함은 그냥 여기서는 스텝이라고 생각을 해 주셔도 될 거 같아요. 몇 번의 스텝, 몇 번의 포워드가 일어나느냐로 생각을 해 주시면 될 거 같고, 그래서 사실은 병렬적으로 처리될 수 있다라는 점에서 훈련에는 굉장히 유리하지만, 모든 시퀀스가 한 번에 들어오는 훈련 시나리오에선 굉장히 유리하지만, 렝스가 길어지거나 혹은 추론하는 상황에선 굉장히 불리할 수밖에 없습니다. 왜냐면 이렇게 모든 키와 밸류를 메테리얼라이즈 해서 가지고 있어야 하기 때문이겠죠. 당연히.

(1:03:05)


그리고 리커런트 폼의 경우에는 전혀 반대 장단점을 가지고 있죠. t번째 메모리를 알기 위해서는 1번째 시점부터 t-1번째 시점까지의 모든 스테이트에 대한 연산이 다 이루어졌었어야 t번째 스테이트를 확보를 할 수 있기 때문에 선행 연산이 이렇게 많이 존재한다라는 것을 확인을 할 수가 있고요. 그래서 병렬 연산이 당연히 불가능하고. 하지만 어떤 불필요한 연산, 앞에서 본 것처럼 앞에 있는 쿼리가 뒤에 있는 키에 어텐드를 하는, 일어나지 않을 것에 대한 연산은 일어나지 않는다 라는, 뭐 그런 점으로 미루어 보아서, 스텝 그러니까 포워드는 많이 일어나지만 플롭스는 적다, 낭비되는 연산은 적다 라고 정리를 드릴 수가 있겠습니다. 그래서 추론에 유리하지만 훈련에는 분리할 수밖에 없겠죠.

(1:03:58)


그리고 여기서부터 조금 머리가 아파지기 시작하는데요. 그 사실은 좋은 건데, 그러니까 리니어어텐션이 듀얼 폼을 가지고 있다 보니까 둘의 장점만을 합친 중간 정도의 병렬 연산도 가능하게 됩니다. 그러니까 시퀀스 랭스가 이렇게 쭉 있을 때 이거 하나하나를 리커런트 하게 하는 방법과, 전체 시퀀스 랭스를 한 번에 포워드를 통해서 병렬 연산하는 것 사이, 그러니까 시퀀스 랭스를 기준으로 해서 청킹을 해서, 그 모든 토큰들을 일정한 크기의 청크로 나눠서 그 청크 내에서는 패럴렐하게 계산을 하고 청크 간의 연산은 리커런트하게 계산을 하는 방법이 있습니다.

예를 들면 왼쪽 아래처럼 나눠졌다 라고 했을 때, 세 개의 청크 각각에 대한 연산은 패럴렐 하게 한 다음에, 그러니까 두 개씩 짝지어져 있는 것들 안에서는 패럴렐하게 연산이 진행이 되고요. 청크 간의 연산 그러니까 세 개의 덩어리 간의 연산은 리커런트하게 진행이 된다라고 생각을 해 주시면 좋을 것 같습니다.

좀 더 자세하게 예를 들어서, 저 세 번째 청크에 대한 연산이 어떻게 수행이 되는지를 살펴보자면, 세 번째 청크에 대한 연산을 한다라는 말은 왼쪽 아래 있는 그림처럼 세 번째 통 청크에 인풋이 들어왔을 때 저번 세 번째 통 청크의 아웃풋이 어떻게 계산될지를 확인을 하면 되는 것이고, 이때 어텐션의 아웃풋이라함은 어떤 키 토큰, 현재 뭐 현재 청크에 들어 있는 쿼리 토큰이 이 모든 키 토큰과 모든 밸류 토큰을 통해서 이제 어그리게이트를 해서 정보를 가지고 오는 것 또한 아웃풋이고, 앞에서 본 것처럼 지금까지에 쌓아져 있는 스테이트에 접근을 해서 쿼리가 적합한 정보를 가지고 오는 것으로도 생각을 할 수 있겠죠. 청크 내에서는 전자의 방법, 그러니까 청크 내에 있는 키와 밸류를 가지고 패럴렐하게 연산을 진행을 하게 됩니다. 그니까 패럴렐 폼으로 연산을 진행을 하게 되고요. 이렇게 하면 이 쿼리가 현재 청크 내에 있는 어떤 키와 현재 청크 내에 있는 밸류로부터 정보를 가지고 오는 연산을 수행을 하게 됩니다.

그런데 사실은 저희는 이 청크 이전 청크들에서도 정보를 가지고 와야 하는 것이고, 그 정보들이 이런, 여기 보시는 것처럼 S4라는 스테이트에 저장이 되어 있다라고 생각을 해 보겠습니다. 그니까 이전 청크의 메모리에 이제 접근을 한다라고 생각을 해 주시면 될 거 같고.

(1:06:33)


그러면 이러한 이전 청크까지의 정보라는 것을 어떻게 구할 수 있겠느냐라고 하면 요런 식으로 구할 수 있겠죠. 이거 자체도 청크 안에서 만들어지는 정보는 이렇게 청크 내에 있는 키와 밸류의 매트릭스 매트릭스 멀티플리케이션을 통해서 만들 수가 있을 것이고요. 저희가 필요한 것은 그러니까 이전 스텝까지 모아져 있던 그 청크까지의 메모리만 가지고 올 수 있게 되면은, 그것에다가 그냥 현재 청크에서 만들어진 방금 만든 저 메모리를 더하는 것으로 업데이트를 시켜 줄 수가 있겠죠.

(1:07:10)


이 파트가 좀 직관적으로 와닿지 않으실 수도 있을 것 같은데, 반대로 스테이트를 어떻게 구축할 것인지부터 생각을 해 보면, 스테이트라는 것은 앞에서 말씀드렸던 것처럼 VK의 곱으로 만들어지는 아우터 프로덕트 한 장 한 장씩을 그냥 이렇게 쭉 더한 것이다라고 생각을 할 수가 있다고 말씀을 드렸었죠.

그래서 만약에 청크 사이즈가 2이다 라고 하면 2씩 만들어진 이 묶음끼리의 매트릭스 매트릭스 멀티플리케이션을 통해서 이런 식으로 어떤 특정 청크에서 더해져야 하는 정보들을 미리 만들어 놓을 수가 있게 되고요. 그렇게 만들어지게 되면 오른쪽에 보시는 것처럼 리커시브 하게 첫 번째 토큰에 대한 정보가 이제 두 번째 토큰 저번 두 번째 청크로 흘러 들어가게 되고, 거기서 이제 두 번째 청크까지 어큐뮬레이트가 된 이제 스테이트를 만들 수가 있고, 그게 또 다음 청크로 넘어가게 되면, 해당 청크에 대해, 해당 청크까지 쌓이게 되는 정보를 담고 있는 스테이트가 만들어질 수가 있다라고 정리를 드릴 수가 있겠습니다.

(1:08:19)


그래서 결론적으로 각 청크까지의 정보가 담겨 있는 어떤 스테이트 S[i]와 같은 스테이트는 만드는 것이 어렵지 않다라는 걸 보여 드렸었고요. 그래서 결론적으로는 어떤 i번째 청크에 대한 아웃풋인 o[i]를 어떻게 만드는지를 살펴보자면, 일단은 현재 청크로부터 정보를 얻는 방법은 요렇게 어텐션을 통해서 얻게 된다라고 말씀을 드렸었고

(1:08:46)


이전 청크로부터 정보를 얻어 와야 되는 부분은 앞서서 만들어 놓은 저 청크들에 대한 스테이트인 S[i], 여기서는 S[i-1]이 되겠죠. S[i-1]의 쿼리를 이제 곱하게 되어서 얻게 되는 정보 이렇게 두 가지를 더하게 되어서 최종적인 아웃풋을 만들 수가 있다 라는 부분을 설명드릴 수가 있을 것 같습니다.

(1:09:15)


그런데 이렇게 리니어어텐션을 변형시켜서 테스트타임 트레이닝을 할 수 있게끔 모델화를, 스테이트를 모델화를 시켜 버리면, 형태가 복잡해지게 되면서 학습의 병렬화가 불가능해지게 됩니다. 그러니까 좀 더 정확하게는, 스테이트를 업데이트 하던 룰이 이전 타임스텝의 메모리에 디펜던트 해지기 때문에 그러한데요.

그니까 왼쪽 저 빨간 박스를 보시면 원래의 업데이트 룰은 이전 스텝의 메모리, 즉 S_t-1에 디펜던트 하지 않죠. 항상 계속 그냥 저 Vt Kt를 더하는 식으로 진행이 되는 걸 확인을 할 수가 있는데, 오른쪽에 보시면 저희는 이제 저 S를 더 이상 스테이트로 두지 않고 모델로 바꾸게 되었고, 그렇기 때문에 저희는 이전 타임스텝에 대한 정보를 담고 있는 메모리 모듈에 지금의 정보를 넣고 그레디던트 디센트를 해야 됩니다. 그러니까 이전 타임스텝에 대한 메모리가 있어야 하는 식으로 바꿔 버렸기 때문에 앞에서 사용을 했던 청크와이즈 패럴렐하게 연산을 할 수 있는 그런 메리트가 없어졌다라고 생각을 할 수가 있죠.

(1:10:25)

이런 타임 디펜던스 때문에 완전한 병렬화는 불가능해졌지만, 사실은 이거를 앞에서 본 리니어어텐션처럼 청크와이즈하게는 어느 정도 패럴렐 하게 할 수 있다라는 것을 사실 ttt 그러니까 “learning to learn at test time”의 저자들이 발견을 해서 청크와이즈 패럴렐 한 훈련을 통해서 모델을 학습을 시켰고요. 그것에 좀 더 계량된, 그것에 계량된 버전으로 타이탄에서도 학습을 수행한다라고 생각을 해 주시면 될 거 같습니다.

(1:10:59)


그래서 이 learning to learn test time의 훈련 병렬화 스킴을 좀 소개를 드리려고 합니다. 사실 이것과 타이탄에서 사용한 것이 거의 동일해서 요걸 가지고 설명드리는게 좀 더 직관적일 것 같아요. 결국에는 왼쪽에 있는 것이 앞에서 본 리니어 어텐션의 청크와이즈 패러렐이고, 이것을 오른쪽에 보이는 저희가 앞서 봤던 learning to learn at test time 레이어에 적용을 시키려고 하는 것이 저의 목적이 되는 것입니다.

(1:11:30)


그래서 리니어어텐션에 어떤 점이 청크와이즈 패럴렐을 가능게 했냐를 살펴보면은, 일단 저 클로즈드 폼으로 구해지는 그레디언트의 해가 패럴렐리즘을 가능케 하는 너무 좋은 요소 중에 하나였죠. 그러니까 VK트랜스포즈라는 연산 자체를 병렬적으로 수행을 할 수 있기 때문에, 그러니까 다른 어떤 정보에 디펜던트하지 않고 그 블록에서 한 번에 계산을 할 수 있었기 때문에 병렬 연산이 가능했다라는 점이 되게 중요했고요.

(1:12:02)


그런데 여기서 보면, t 시점의 모델은 결국 t-1 시점의 정보가 있어야 업데이트를 할 수 있었기 때문에, 결국 이 정보 계산이 리컬시브 하니까 어떻게 업데이트 할 정보를 패럴렐하게 하지?에 대한 고민이 필요하게 되는 것입니다.

(1:12:22)


그런데 식을 어떤 메모리를 업데이트하는 식이 아니라, 그냥 어떤 임의의 로스를 가지고 모델을 업데이트 시키는 식으로 그냥 단순하게 놓고 생각을 해 보면, 이 식은 사실은 모델이 하나의 배치의 샘플을 받아서 업데이트가 되고 있는 상황인 걸로 이해를 할 수가 있죠.

근데 이거를 사실은 저희는 임의의 로스라면 그냥 이 모델에 대해서 배치 그레디언트 디센트를 통해서 모델을 최적화를 하더라도 뭐 안 될 거는 없겠죠. 그래서 이 식 자체를 배치 크기가 C인, 그러니까 C 크기의 배치를 가지고 그레디언트 디센트를 하는 식으로 아래처럼 바꿔 놓고 보면, 청크와이즈하게 패럴렐한 연산을 하는 것으로 또 바꿔서 생각을 해 볼 수가 있습니다. 그러니까 어떻게 보면 C 크기의 배치 내에 있는 것들을 모두 병렬화를 해서 연산을 한다라고 진행을 했을 때, 저의 기준이 되는 어떤 모델 그러니까 C스텝 전의 모델이 되겠죠, t-c 라는 시점의 모델 에다가, 원래라면 저기에 저 파란색 박스의 제일 아래쪽에 있는 식이 되겠죠, 저기에 들어가야 되는 저기에 대해서 검증받아야 되는 정보는 사실 t-c+1 시점의 정보가 들어가서 그레디언트 디센트를 했어야 됐을 것입니다.

근데 그것뿐만이 아니라 쭉쭉쭉 위로 올라가서 저 시점의 모델에다가 X를 놓고 이 X를 얼마나 잘 리컨스트럭션을 하느냐로 만들어진 로스를 사용할 수 있다라는 것이죠. 그러니까 사실 메모리 관점에서 보면 이제, 어떻게 보면 서프라이즈 매트릭을 측정을 하게 되는 기준이 바로 전 시점의 메모리가 아니라 최대 C시점 이전 메모리의 어떤 복원 능력을 기준으로 서프라이즈 매트릭을 측정을 한다, 즉 로스를 측정한다 라고 생각을 해 주시면 될 거 같습니다.

(1:14:24)

이렇게 되면 사실 청크와이즈 패럴렐도 가능하고, 왜냐하면 하나의 청크 안에 있는 C개 토큰에 대해서 모든 그레디언트를 한 번에 구할 수 있기 때문에 청크와이즈 패럴렐이 되는 것이고요. 동시에 뭐 배치와이즈 그레디언트 디센트를 통해서 업데이트 한다라고도 해석할 수 있겠죠.

(1:14:45)

중요한 점은 여기서 업데이트 룰은 그대로 리커런트하다 라는 점을 말씀드리고 싶습니다. 그러니까 맨 위에 있는 식을 보시자면 저 M_t 는 M_(t-c) 에다 저 로스가 더해지는 것이 아니라, M_(t-1) 은 그대로지만 로스 텀만, 아까 업데이트되는 저 로스 텀만, 마이너스 델타 로스(M_(t-c),x_t) 가 된다 라고 말씀을 드릴 수가 있겠습니다.

(1:15:13)


그러면 스테이트는 이렇게 만들었다라고 가정을 했을 때 뒤에는 어떤 일이 발생을 하게 되냐면, 일단은 기존의 리니어어텐션의 경우에는 아웃풋을 계산하는 것 자체도 이런 식으로 패럴렐하게 연산을 할 수가 있었죠.

(1:15:30)


그런데 문제는 이 TTT 레이어의 아웃풋 계산은 여전히 리커런트로 남아 있다 라는데 있습니다. 좀 더 자세히 말씀드려보면, 저희가 병렬적으로 처리해야 할 이 오른쪽 아래에 보이시는 이 C개의 X 토큰이 있고, 이것들 각각이 어떤 모델에 통과되어야 되는지에 대한 로스 결과값도 나와 있습니다. 근데 문제는 저 각각을 통과시켜야 되는 모델이 각각 C개이기 때문에 실제로 C개의 모델을 만들어 내야함에 있겠죠.

근데 이것은 저희가 바라는 상황은 아닐 것입니다. 예를 들면서 청크의 사이즈가 1024나 이렇게 크다면, 저희가 구한 각각의 1024개의 로스 텀을 가지고서 실제로 1024개의 모델을 메테리얼라이즈를 한 다음에 각각의 X 토큰들을 각각의 모델에 포워딩을 시켜야겠죠.

이러면 이제 패럴렐하게 연산이 안 되는 것이니까, 결국에 저희의 리서치 퀘스천은, 제일 첫 청크라고 생각을 했을 때, 이 M0 그러니까 가장 초기의 모델 혹은 바로 이전 청크까지의 정보가 업데이트된 모델 M0가 있고, 그걸로부터 이제 저희가 배치 그레디언트 디센트를 통해서 만들어 낸 C개의 로스 결과가 있을 때, 모든 시점에 대한 모델을 실제로 만들지 않고 이 C개의 쿼리에 대한 포워드 결과를 계산할 수 있는 방법이 있을까 에 대해서 고민을 해 보아야 합니다.

(1:17:08)


이걸 가능하게 하는 방법이 어떤 로스를 잘 디컴포즈를 해서 따로따로 업데이트를 시키는 어떤 방법이다 라고 말씀을 드릴 수가 있겠고요.

어 조금 복잡하긴 한데 이렇게 말씀을 드려 보도록 하겠습니다. 일단 청크 사이즈가 4라고 생각을 해 보면, 지금 동시에 처리해야 되는 어떤 토큰의 수가 네 개다 라는 뜻이 되겠죠. 일단 로스 계산부터 생각을 해 보면, 저희는 이제 이렇게 생긴 MLP 레이어, 그러니까 지금 첫 번째 레이어와 두 번째 레이어 시작 부분만 이렇게 나와 있는 상태라고 생각을 해 보겠습니다. 그러면 이렇게 생긴 MLP 레이어에 네 개의 키 토큰을 집어넣고 네 개의 밸류 토큰과의 리컨스트럭션 로스를 통해서 얻어지는 네 개의 어떤 개별적인 로스를 얻을 수가 있게 되겠죠. 이 로스가 계산되고 나면 일단은 백프로파게이션을 해서 저 ẑ_1 까지의 어떤 편미분 값을 계산해 두고 있는 상태다라고 생각을 해 보겠습니다.


(1:18:16) 그러면 저 네 개의 로스들 중에 저 L3의 관점에서 한번 생각을 해 봅시다. 그러니까 세 번째 키 토큰이 모델에 들어가서 세 번째 밸류 토큰을 리컨스트럭션을 하는 것에 의해서 생긴 로스는 결국에 그 체인룰을 풀어 보면은 저렇게 쓸 수가 있겠죠.


(1:18:38) 그리고 나서 이제 저희가 포워딩을 시켜야 될 각 쿼리를 통과시킬 건데, 저 q3 입장에서 보면은 사실 일종에 LoRA에서 했던 거랑 비슷한데요. 저희가 로라에서 어떤 업데이트 될 모델의 상태를 업데이트 되기 전 그냥 프리트레인드만 됐던 모델의 웨이트 더하기 웨이트의 람다로 디컴포즈를 해서 이렇게 하는 접근을 취하잖아요. 여기 아래쪽에 있는 것도 그것과 마찬가지로 이 식이 말하고자 하는 건 저 아랫 부분에 저 식에서 빨간색으로 표시된 부분은 저 쿼리가 업데이트 되지 않은 모델에 통과해서 생기는 어떤 텐서를 뜻하게 되는 것이고요. 오른쪽은 저게 s가 1부터 t까지 시그마로 이루어져 있으니까, 예를 들면 여기 지금 세 번째 토큰이면 첫 번째 토큰에 대한 리컨스트럭션 로스, 두 번째 토큰에 대한 리컨스트럭션 로스, 세 번째 토큰에 대한 리컨스트럭션 로스로 인해서 모델이 나중에 업데이트가 될 텐데, 그 업데이트가 될 포션 만큼으로 인해서 만들어지는 웨이트의 쿼리가 통과해서 나오는 값을 뜻한다 라고 말씀드릴 수가 있겠습니다.


(1:19:45) 그래서 사실 이거를 배치 단위로 일반화를 하면 요런 식으로 바뀌게 되겠죠. 바뀌게 되는데 사실 이렇게 되면 모든 토큰이 모든 로스에 대해서 영향을 받게 되는 것이고, 사실 저희는 이걸 원하지 않으니까..


(1:20:03) 이거를 해결을 하기 위해서 마치 어텐션 연산에서 커설 마스크를 씌우는 것처럼 앞에 쿼리가 들어갔을 때에 대한 연산에는 뒤에 토큰들에 대한 키밸류 리스컨스트럭션으로부터 나오는 로스의 영향을 받지 않게끔 하기 위해서 저렇게 마스킹을 씌워 주는 것을 확인을 할 수가 있습니다.

이렇게 커슬 마스크를 사용을 하면 최종적인 패리즘 수식이 완성이 되고요. 이건 이제 하나의 매트릭스에 대해서 일어나는 것이니까 이게 이 MLP 레이어에 있는 전체 매트릭스에 대해서 일어난다라고 생각을 해 주시면 될 거 같습니다.


(1:20:37) 그래서 결론적으로는 배치 그레디언트 디센트를 통해서 모델을 얼만큼 업데이트를 해야 될지, 업데이트 해야 되는 그 포션을 이제 배치 내에서 그러니까 청크 내에서 패럴렐하게 연산을 할 수가 있었고요. 더해서 아웃풋 계산도 이런 식으로 청크 내에서는 패럴레하게 할 수 있게끔 만들어진 것을 확인을 할 수가 있습니다. 그래서 이러한 두 가지 방식을 사용을 해서 리니어어텐션에서 했던 것처럼 인트라 청크에서는 패럴렐하게 그리고 인터청크에서는 리커런트하게 하는 이런 연산을 구현했다라고 정리를 드릴 수가 있을 것 같습니다.

## Closing

(1:21:15) 네. 여기까지 해서 오늘 세미나에 대한 마무리를 좀 진행을 해 보고자 하는데요.


그래서 오늘 살펴본 내용들과 결이 맞다 있는 퓨처웍스 혹은 지금 진행되고 있는 연구들의 결이 어떤지 좀 말씀을 드리자면 제가 파악한 바로는 사실 이런게 제가 계속 논문 세미나 등에서 말씀을 드렸던 뭐 맘바나 요즘 많이 유명한 라쿠브 같은 것과도 사실 결이 맞닿아있다라고 생각을 할 수가 있죠. 이런 것들이 사실은 다 하드웨어 에피션트하게 어떻게 리니어 어텐션을 구현을 할 것인가와 맞닿아 있는 부분이다라고 말씀드릴 수가 있을 것 같고요. 그리고, 세 번째 꼭지부터 말씀을 드리자면, 소프트맥스 연산이 어텐션에서 매우 중요하다라는 걸 확인을 했기 때문에 이런 소프트맥스 자체를 O의 n제곱에서 좀 밑으로 끌어내리고 오면서도 그의 익스프레시브 한 파워를 그대로 가지고 가게 할 수 있는 어떤 커널이 있을까에 대한 연구도 많이 진행이 되는 것을 확인을 했습니다. 그 결국에는 중요한 건 소프트맥스를 어떤 커널이 소프트맥스를 근사하면서도 온라인하게 그러니까 리니어하게 사용이 될 수 있는 미분 가능한 그런 존재여야 한다라는 어떤 조건 하에서 그럼 어떻게 설계를 해야 될까 요런 거에 대한 연구도 좀 많이 이루어지는 거 같고요.

(1:22:42) 사실 이 시퀀스 모델링의 theory of everything 2화가 나온다면 이건 아마 어소시어티브 메모리 관련된 내용이 아닐까라는 생각이 듭니다. 그러니까 결국에는 오늘 제가 보여 드린 것은 단지 어떤 리니어 어텐션과 테스트타임 트레이닝에 대한 연결 고리만을 보여 드렸지만, 사실 이것들 모두를 아우르는 Theory of Everything 과 같은 연구들이 나오고 있다라고 말씀을 드렸고, 그 논문들이 사실은 이 시퀀스 모델링 이라는 것을 associative 메모리에 대한 구현의 일종이다 라고 바라보고 있고, 거의 모든 논문들이, 그런 관점에서 봤을 때 리니어어텐션과 테스트타임 트레이닝 모두를 어떻게 설명을 할 수 있는 어떤 집대성 격의 이론이 나올 수 있다라고 생각을 해서 그런 연구들이 좀 계속 나오고 있는 것 같습니다. 그럼 어소시어티브 메모리라 함은 어떤 키 밸류의 맵핑으로 말씀을 드릴 수가 있는데, 단지 어떤 이런 키 밸류의 매핑이라는 컨셉으로부터 출발해서 되게 오늘날 많이 이루어지고 있는 이런 연구들까지도 확장해서 설명을 할 수 있는 그런 개념이 이미 조금씩 드러나는 것 같고요. 그래서 그러한 관점에서 뭐 그런 것도 있습니다. 이 어소셔티브 메모리의 어떤 가장 대표적인 구현체인 호필드 네트워크의 관점에서 역으로 이렇게 소프트맥스 기반의 어텐션을 설명을 하면서 그래서 소프트맥스 어텐션 시퀀스 모델링을 하는 소프트맥스 어텐션도 결국 호필드 네트워크의 어떤 특수 케이스이다라는 설명과 동시에 그래서 이러 이러한 어소셔티브 메모리의 작동 방식을 고려했을 때 요렇게 바꾸면 더 성능이 좋아질 것이다. 뭐 요런 연구들을 통해서 실제로 성능을 높이는 연구들도 많이 진행이 되고 있다. 그렇기 때문에 어 다음번 세미나에서는 아마 어소티브 메모리를 다루지 않을까라는 생각이 좀 드는 거 같습니다.

(1:24:46) 네. 내용이 워낙 방대하다 보니까 전달드리고 싶은 내용이 그만큼 너무 많았던 것 같은데요. 그래서 오히려 전달이 또 다시 제대로 되지 않은 것은 아닌가 하는 느낌이 드는 거 같습니다. 항상 발표가 끝나고 나면 그런 느낌이 드는 것 같고요. 근데 사실 타이탄이라는 논문을 리뷰를 하고 이걸 좀 제대로 알기 위해서는 저 모든 내용들이 사실은 다 필요한 내용이다라고 저는 생각이 듭니다. 그래서 제 발표가 좀 별로였더라도 이 타이탄이라는 논문, 좀 핫했던 논문을 제대로 이해하고 싶으신 분들이라면 제가 제시한 토픽을 위주로라도 한번 다시 보시고 타이탄이라는 논문을 보시면 좀 더 이해가 잘 되지 않을까라는 생각도 드는 거 같습니다.

그래서 언제나 그렇듯이 궁금하신 것들이나 같이 이야기해 보고 싶은 것들 있으시다면 댓글로 항상 남겨 주시면 감사드리겠습니다. 아는 선에서 최대한 얘기를 나눠보면 정말 재밌을 것 같고요. 네. 그럼 긴 발표 들어 주셔서 감사합니다.